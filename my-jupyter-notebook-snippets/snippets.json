{
    "snippets" : [
        {
            "name" : "Warning filters",
            "code" : [
                "import warnings",
                "",
                "warnings.filterwarnings('ignore', category = FutureWarning)",
                "warnings.filterwarnings('ignore', category = UserWarning)"
            ]
        },
        {
            "name" : "Google Drive disk mount",
            "code" : [
                "from google.colab import drive",
                "",
                "drive.mount('/content/drive')",
                "drive_path = 'drive/MyDrive'"
            ]
        },
        {
            "name" : "Compress notebook",
            "code" : [
                "import os",
                "from ipynbcompress import compress",
                "",
                "def compress_notebook(file_in, file_out):",
                "    old_size = os.stat(file_in).st_size",
                "    new_size = compress(file_in, output_filename = file_out, img_format = 'jpeg')",
                "    return old_size - new_size"
            ]
        },
        {
            "name" : "Images quality tweaks",
            "code" : [
                "# image format",
                "%config InlineBackend.figure_format = 'jpg'",
                "# dpi tweak (default - 100)",
                "matplotlib.rcParams['figure.dpi'] = 100",
                "# plot size tweak",
                "matplotlib.pyplot.rcParams['figure.figsize'] = (10, 3) # makes figures larger"
            ]
        },
        {
            "name" : "Pandas, NumPy, Matplotlib, Seaborn, etc",
            "code" : [
                "import numpy as np",
                "import pandas as pd",
                "pd.set_option('display.max_rows', 50)",
                "pd.set_option('display.max_columns', 10)",
                "import matplotlib.pyplot as plt",
                "import seaborn as sb",
                "import missingno as msno",
                "import random as rd"
            ]
        },
        {
            "name" : "Scikit-learn utilities",
            "code" : [
                "from sklearn.metrics import confusion_matrix, classification_report",
                "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder",
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "name" : "Classifiers (Scikit-learn, CatBoost, XGBoost)",
            "code" : [
                "from sklearn.model_selection import GridSearchCV",
                "from sklearn.neighbors import KNeighborsClassifier",
                "from sklearn.tree import DecisionTreeClassifier",
                "from sklearn.linear_model import LogisticRegression",
                "from sklearn.linear_model import SGDClassifier",
                "from sklearn.svm import LinearSVC",
                "from sklearn.svm import SVC",
                "from sklearn.ensemble import RandomForestClassifier",
                "from sklearn.ensemble import ExtraTreesClassifier",
                "from sklearn.ensemble import BaggingClassifier",
                "from sklearn.ensemble import AdaBoostClassifier",
                "from sklearn.ensemble import GradientBoostingClassifier",
                "from sklearn.ensemble import HistGradientBoostingClassifier",
                "from sklearn.ensemble import VotingClassifier",
                "from sklearn.naive_bayes import GaussianNB",
                "from catboost import CatBoostClassifier",
                "from xgboost import XGBClassifier"
            ]
        },
        {
            "name" : "Regressors & metrics (Scikit-learn, CatBoost, XGBoost)",
            "code" : [
                "from sklearn.neighbors import KNeighborsRegressor",
                "from sklearn.linear_model import LinearRegression",
                "from sklearn.tree import DecisionTreeRegressor",
                "from sklearn.ensemble import RandomForestRegressor",
                "from sklearn.ensemble import ExtraTreesRegressor",
                "from sklearn.ensemble import BaggingRegressor",
                "from sklearn.ensemble import AdaBoostRegressor",
                "from sklearn.ensemble import HistGradientBoostingRegressor",
                "from catboost import CatBoostRegressor",
                "from xgboost import XGBRegressor",
                "from sklearn.model_selection import GridSearchCV",
                "from sklearn.metrics import r2_score",
                "from sklearn.metrics import mean_absolute_error",
                "from sklearn.metrics import mean_squared_error"
            ]
        },
        {
            "name" : "TF & Keras",
            "code" : [
                "import tensorflow as tf",
                "from tensorflow import keras",
                "from tensorflow.keras.utils import to_categorical",
                "from keras.callbacks import EarlyStopping, ModelCheckpoint",
                "from keras.models import Sequential",
                "from keras.layers import Dense, Flatten, Dropout, Conv2D, BatchNormalization, ReLU",
                "from keras.layers import MaxPooling2D, AveragePooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D",
                "# Image preprocessing",
                "from keras.layers import Rescaling, Resizing, CenterCrop",
                "# Image augmentation",
                "from keras.preprocessing.image import ImageDataGenerator",
                "from keras.layers import RandomCrop, RandomFlip, RandomTranslation, RandomRotation, RandomZoom, RandomContrast",
                "",
                "# List of devices",
                "tf.config.list_physical_devices()",
                "",
                "# Plot model function",
                "# keras.utils.plot_model(model, show_shapes = True)",
                "",
                "# Model loading function",
                "# model = keras.models.load_model('model.keras', compile = False)",
                "# model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])",
                "# model.summary()"
            ]
        },
        {
            "name" : "PyTorch",
            "code" : [
                "import torch",
                "",
                "torch.cuda.is_available()"
            ]
        },
        {
            "name" : "MNIST digits dataset (Scikit-learn)",
            "code" : [
                "from sklearn.datasets import fetch_openml",
                "from sklearn.preprocessing import LabelEncoder",
                "",
                "data = fetch_openml('mnist_784', version = 1, cache = True, parser = 'auto')",
                "x = data['data']",
                "y = data['target']",
                "",
                "encoder = LabelEncoder()",
                "encoder.fit(y)",
                "print(encoder.inverse_transform([0]))",
                "print(encoder.classes_)"
            ]
        },
        {
            "name" : "MNIST digits dataset (Keras)",
            "code" : [
                "from tensorflow.keras.datasets import mnist",
                "from sklearn.preprocessing import LabelEncoder",
                "",
                "(x_train, y_train), (x_test, y_test) = mnist.load_data()",
                "",
                "encoder = LabelEncoder()",
                "encoder.fit(y_train)",
                "print(encoder.inverse_transform([0]))",
                "print(encoder.classes_)"
            ]
        },
        {
            "name" : "EMNIST digits & letters dataset",
            "code" : [
                "from extra_keras_datasets import emnist",
                "from sklearn.preprocessing import LabelEncoder",
                "",
                "(x_train, y_train), (x_test, y_test) = emnist.load_data(type = 'balanced')",
                "",
                "# labels for the balanced dataset, https://arxiv.org/pdf/1702.05373v1.pdf",
                "labels = [str(i) for i in range(0, 10)]",
                "labels += ['a', 'b', 'd', 'e', 'f', 'g', 'h', 'n', 'q', 'r', 't']",
                "labels += [chr(i) for i in range(65, 91)]",
                "",
                "encoder = LabelEncoder()",
                "encoder.fit(labels)",
                "print(encoder.inverse_transform([0]))",
                "print(encoder.classes_)"
            ]
        },
        {
            "name" : "CIFAR-10 dataset",
            "code" : [
                "from keras.datasets import cifar10",
                "from sklearn.preprocessing import LabelEncoder",
                "",
                "(x_train, y_train), (x_test, y_test) = cifar10.load_data()",
                "",
                "labels = ['airplane', 'automobile', 'bird',",
                "          'cat', 'deer', 'dog', 'frog',",
                "          'horse', 'ship', 'truck']",
                "encoder = LabelEncoder()",
                "encoder.fit(labels)",
                "print(encoder.inverse_transform([0]))",
                "print(encoder.classes_)"
            ]
        },
        {
            "name" : "Image dataset showcase",
            "code" : [
                "def showcase_image_dataset(x, y, encoder = None):",
                "    plt.subplots(4, 5, figsize = (8, 5))",
                "    plt.tight_layout()",
                "    for i in range(0, 20):",
                "        n = np.random.randint(0, x.shape[0])",
                "        plt.subplot(4, 5, i + 1)",
                "        plt.axis('off')",
                "        if encoder == None:",
                "            plt.gca().set_title(f'{y[n]}')",
                "        else:",
                "            plt.gca().set_title(f'{encoder.inverse_transform([y[n]])[0]} ({y[n]})')",
                "        plt.imshow(x[n])",
                "    plt.show()"
            ]
        },
        {
            "name" : "Show class distribution",
            "code" : [
                "def show_class_distribution(y, encoder = None):",
                "    ticks = [i for i in range(0, len(y.unique()))]",
                "    plt.figure(figsize = (10, 4))",
                "    plt.title('Class distribution')",
                "    if encoder != None:",
                "        sb.histplot(encoder.inverse_transform(y), discrete = True)",
                "    else:",
                "        sb.histplot(y, discrete = True)",
                "    plt.xticks(ticks)",
                "    plt.show()"
            ]
        },
        {
            "name" : "Show confusion matrix",
            "code" : [
                "def show_confusion_matrix(labels_test, labels_predicted, figure_size = (10, 7), labels = None):", 
                "    plt.figure(figsize = figure_size)",
                "    cm = confusion_matrix(labels_test, labels_predicted)",
                "    if labels != None:",
                "        sb.heatmap(",
                "            cm,",
                "            annot = True,",
                "            fmt = 'd',",
                "            cbar = False,",
                "            xticklabels = labels,",
                "            yticklabels = labels)",
                "    else:",
                "        sb.heatmap(",
                "            cm,",
                "            annot = True,",
                "            fmt = 'd',",
                "            cbar = False)",
                "    plt.xlabel('Predicted')",
                "    plt.ylabel('Truth')",
                "    plt.show()"
            ]
        },
        {
            "name" : "Show regression chart",
            "code" : [
                "def show_regression_chart(y, prediction, sort = True):",
                "    t = pd.DataFrame({'prediction': prediction, 'y': y})",
                "    if sort:",
                "        t.sort_values(by = ['y'], inplace = True)",
                "    plt.figure(figsize = (10, 6))",
                "    plt.plot(t['prediction'].tolist(), label = 'prediction')",
                "    plt.plot(t['y'].tolist(), label = 'expected')",
                "    plt.ylabel('output')",
                "    plt.legend()",
                "    plt.show()"
            ]
        },
        {
            "name" : "Show training history",
            "code" : [
                "def show_training_history(history, metrics, metric_names, figure_size = (7, 6), last_epoch = None):",
                "    if len(metrics) == 1 and len(metric_names) == 1:",
                "        plt.figure(figsize = figure_size)",
                "        fig, ax = plt.subplots()",
                "        metric = metrics[0]",
                "        metric_name = metric_names[0]",
                "        ax.plot(history[metric], color = 'b', label = f'Training {metric_name}')",
                "        if f'val_{metric}' in history:",
                "            ax.plot(history[f'val_{metric}'], color = 'r', label = f'Validation {metric_name}')",
                "        if last_epoch is not None:",
                "            ax.axvline(x = last_epoch, color = 'g', label = 'Start of fine tuning')",
                "        legend = ax.legend(loc = 'best', shadow = True)",
                "        plt.show()",
                "    elif len(metrics) > 1 and len(metrics) == len(metric_names):",
                "        plt.figure(figsize = figure_size)",
                "        fig, ax = plt.subplots(len(metrics), 1)",
                "        for i in range(len(metrics)):",
                "            metric = metrics[i]",
                "            metric_name = metric_names[i]",
                "            ax[i].plot(history[metric], color = 'b', label = f'Training {metric_name}')",
                "            if f'val_{metric}' in history:",
                "                ax[i].plot(history[f'val_{metric}'], color = 'r', label = f'Validation {metric_name}', axes = ax[i])",
                "            if last_epoch is not None:",
                "                ax[i].axvline(x = last_epoch, color = 'g', label = 'Start of fine tuning')",
                "            legend = ax[i].legend(loc = 'best', shadow = True)",
                "        plt.show()",
                "    else:",
                "        raise Exception('Invalid metrics/metric names amount')"
            ]
        },
        {
            "name" : "Remove outliers",
            "code" : [
                "from scipy import stats",
                "",
                "def remove_outliers(df, column):",
                "    q_low = df[column].quantile(0.01)",
                "    q_high = df[column].quantile(0.99)",
                "    return df[(df[column] < q_high) & (df[column] > q_low)]",
                "",
                "def remove_outliers_iqr(df, column):",
                "    q1 = np.percentile(df[column], 25, interpolation = 'midpoint')",
                "    q3 = np.percentile(df[column], 75, interpolation = 'midpoint')",
                "    iqr = q3 - q1",
                "    lower = q1 - 1.5*iqr",
                "    upper = q3 + 1.5*iqr",
                "    upper_array = np.where(df[column] >= upper)[0]",
                "    lower_array = np.where(df[column] <= lower)[0]",
                "    result = df.copy()",
                "    result.drop(index = upper_array, inplace = True)",
                "    result.drop(index = lower_array, inplace = True)",
                "    return result",
                "",
                "def remove_outliers_z_score(df, column):",
                "    result = df.copy()",
                "    result['z_score'] = stats.zscore(df[column])",
                "    result = result.loc[result['z_score'].abs() <= 3]",
                "    return result"
            ]
        },
        {
            "name" : "T-test (example)",
            "code" : [
                "from scipy.stats import ttest_ind",
                "",
                "tval, pval = ttest_ind(",
                "    df.loc[df['Role'] == 'First_Baseman', ['Height']],",
                "    df.loc[df['Role'] == 'Second_Baseman', ['Height']],",
                "    equal_var = False)",
                "print(f'T-value: {tval[0]:.2f}')",
                "print(f'P-value: {pval[0]}')"
            ]
        },
        {
            "name" : "(Pandas) Show top classes",
            "code" : [
                "def show_top_classes(df, column, num_of_top_classes = None):",
                "    top = df[column].value_counts()",
                "    if num_of_top_classes != None:",
                "        top = top[:num_of_top_classes]",
                "    plt.figure(figsize = (10, 7))",
                "    sb.barplot(x = top.index, y = top.values)",
                "    plt.xticks(rotation = 45)",
                "    plt.title('Top classes', color = 'blue')",
                "    plt.show()"
            ]
        },
        {
            "name" : "(Pandas) Dataset lookup",
            "code" : [
                "def dataset_lookup(df):",
                "    print('Dataset info:')",
                "    print(df.info())",
                "    print()",
                "    print('Dataset description:')",
                "    print(df.describe())",
                "    print()",
                "    print('Amount of duplicates:')",
                "    duplicates_amount = df.duplicated().sum()",
                "    print(duplicates_amount)",
                "    if (duplicates_amount > 0):",
                "        print('Duplicates:')",
                "        print(df[df.duplicated(keep = False)])",
                "    print()",
                "    print('Amount of missing values:')",
                "    print(df.isna().sum(axis = 0))",
                "    print()",
                "    print('Unique values in each column:')",
                "    object_types_count = 0",
                "    for column in df:",
                "        if df[column].dtypes == 'object':",
                "            object_types_count += 1",
                "            print('-' * len(column))",
                "            print(f'{column}: {df[column].unique()}')",
                "    if object_types_count == 0:",
                "        print('-' * 10)",
                "        print('No object columns in this dataset.')"
            ]
        },
        {
            "name" : "(Pandas) Dataset preprocessing template",
            "code" : [
                "dataset_preprocessed = dataset.copy()",
                "# Renaming of the values in column",
                "dataset_preprocessed['column_name'].replace({1: 'Class_1', 2: 'Class_2'}, inplace = True)",
                "# One-hot encoding",
                "dataset_preprocessed = pd.get_dummies(dataset_preprocessed, columns = ['column_name'])",
                "dataset_preprocessed.replace({False: 0, True: 1}, inplace = True)",
                "# Label encoding",
                "label_encoder = LabelEncoder()",
                "dataset_preprocessed['column'] = label_encoder.fit_transform(dataset_preprocessed['column'])",
                "# Standardization",
                "dataset_numerical_only = dataset_preprocessed.drop(",
                "    columns = [",
                "        'col_1',",
                "        'col_2',",
                "        'col_3'",
                "    ])",
                "scaler = StandardScaler()",
                "dataset_standardized = pd.DataFrame(",
                "    scaler.fit_transform(dataset_numerical_only),",
                "    columns = [",
                "        'col_1',",
                "        'col_2',",
                "        'col_3',",
                "    ])",
                "dataset_standardized.insert(3, 'old_col_1', dataset_preprocessed['old_col_1'])",
                "dataset_standardized.insert(4, 'old_col_2', dataset_preprocessed['old_col_2'])",
                "dataset_standardized"
            ]
        },
        {
            "name" : "(Pandas) Visualization functions",
            "code" : [
                "def plot_dataset(df):",
                "    df.plot()",
                "    plt.show()",
                "",
                "def plot_dataset_histograms(df):",
                "    df.hist()",
                "    plt.show()",
                "",
                "def show_area_plot(df):",
                "    df.plot.area()",
                "    plt.show()",
                "",   
                "def show_proportional_area_plot(df):",
                "    dfp = df.select_dtypes(exclude = ['object']).iloc[:, :].apply(lambda x: x / x.sum(), axis = 1)",
                "    dfp.plot.area()",
                "    plt.show()",
                "",    
                "def show_correlation_map(df, figure_size = (15, 15)):",
                "    plt.figure(figsize = figure_size)",
                "    corr_matrix = df.select_dtypes(exclude = ['object']).corr()",
                "    lower = corr_matrix.where(np.tril(np.ones(corr_matrix.shape), k = -1).astype(np.bool_))",
                "    sb.heatmap(",
                "        lower,",
                "        annot = True,",
                "        fmt = '.2f',",
                "        cbar = False)",
                "    plt.show()",
                "",
                "def show_pair_plot(df, figure_size = (15, 15), target_name = None):",
                "    plt.figure(figsize = figure_size)",
                "    df_without_object_columns = df.select_dtypes(exclude = ['object'])",
                "    if target_name != None:",
                "        sb.pairplot(df_without_object_columns, palette = 'tab10', hue = target_name)",
                "    else:",
                "        sb.pairplot(df_without_object_columns)",
                "    plt.show()",
                "",    
                "def show_cat_plot(df):",
                "    sb.catplot(df, kind = 'boxen', height = 8)",
                "    plt.show()"
            ]
        },
        {
            "name" : "(Pandas) Dataframe manipulation functions",
            "code" : [
                "def transpose_dataset(df):",
                "    return df.describe().transpose()",
                "",
                "def replace_bool_with_int(df):",
                "    return df.replace({False: 0, True: 1})",
                "",
                "def drop_rows_range(df, start_index, end_index):",
                "    df_dropped = df.drop(dataset.index[start_index:end_index])",
                "    return df_dropped",
                "",
                "def rebuild_index_in_dataset(df):",
                "    return df.reset_index().drop(columns = ['index'])",
                "",
                "def add_rows_to_dataset(df, dataframe_rows):",
                "    return pd.concat([df] + dataframe_rows, ignore_index = True)",
                "",
                "def get_row_by_index(df, index):",
                "    return df.iloc[[index]]"
            ]
        },
        {
            "name" : "Dataset split",
            "code" : [
                "from sklearn.model_selection import train_test_split",
                "from tensorflow.keras.utils import to_categorical",
                "",
                "x_train_, x_test, y_train_, y_test = train_test_split(x, y, test_size = 0.25, random_state = 42, shuffle = True)",
                "x_train, x_validation, y_train, y_validation = train_test_split(x_train_, y_train_, test_size = 0.1, random_state = 42)",
                "",
                "y_train_categorical = to_categorical(y_train)",
                "y_test_categorical = to_categorical(y_test)",
                "y_validation_categorical = to_categorical(y_validation)",
                "",
                "print(f'x train: {x_train.shape}')",
                "print(f'y train: {y_train.shape}')",
                "print(f'y train categorical: {y_train_categorical.shape}')",
                "print()",
                "print(f'x test: {x_test.shape}')",
                "print(f'y test: {y_test.shape}')",
                "print(f'y test categorical: {y_test_categorical.shape}')",
                "print()",
                "print(f'x validation: {x_validation.shape}')",
                "print(f'y validation: {y_validation.shape}')",
                "print(f'y validation categorical: {y_validation_categorical.shape}')"
            ]
        },
        {
            "name" : "One-hot encoding",
            "code" : [
                "dataset_encoded = pd.get_dummies(dataset, columns = ['column'])",
                "dataset_encoded.replace({False: 0, True: 1}, inplace = True)",
                "dataset_encoded"
            ]
        },
        {
            "name" : "Label encoding",
            "code" : [
                "from sklearn.preprocessing import LabelEncoder",
                "",
                "label_encoder = LabelEncoder()",
                "dataset_encoded = dataset.copy()",
                "dataset_encoded['column'] = label_encoder.fit_transform(dataset_encoded['column'])",
                "dataset_encoded"
            ]
        },
        {
            "name" : "LassoCV",
            "code" : [
                "from numpy import arange",
                "from sklearn.linear_model import LassoCV",
                "from sklearn.model_selection import RepeatedKFold",
                "",
                "cv = RepeatedKFold(n_splits = 10, n_repeats = 3, random_state = 1)",
                "model = LassoCV(alphas = arange(0.1, 10, 0.1), cv = cv, n_jobs = -1)",
                "model.fit(x, y)",
                "",
                "print(model.alpha_)"
            ]
        },
        {
            "name" : "ROC curve (two classes)",
            "code" : [
                "from sklearn.metrics import roc_curve, roc_auc_score",
                "",
                "y_pred_proba = model.predict_proba(x_test)[:, 1]",
                "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)",
                "roc_auc = roc_auc_score(y_test, y_pred_proba)",
                "# Plot the ROC curve",
                "plt.plot(fpr, tpr, label = f'ROC curve (area = {roc_auc})')",
                "# ROC curve for tpr = fpr",
                "plt.plot([0, 1], [0, 1], 'k--', label = 'Classifier')",
                "plt.xlabel('False positive Rate')",
                "plt.ylabel('True positive Rate')",
                "plt.title('ROC curve')",
                "plt.legend(loc = 'lower right')",
                "plt.show()"
            ]
        },
        {
            "name" : "ROC curve (multiple classes)",
            "code" : [
                "from sklearn.metrics import roc_curve, roc_auc_score",
                "from sklearn.multiclass import OneVsRestClassifier",
                "",
                "oneVsRest = OneVsRestClassifier(SomeModel(random_state = 42))",
                "oneVsRest.fit(x_train, y_train)",
                "y_pred_proba = oneVsRest.predict_proba(x_test)",
                "roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class = 'ovr')",
                "print(f'ROC-AUC Score: {roc_auc}')",
                "colors = ['red', 'green', 'blue', 'yellow']",
                "classes = ['0', '1', '2', '3']",
                "for i in range(len(classes)): ",
                "    fpr, tpr, thresh = roc_curve(y_test, y_pred_proba[:, i], pos_label = i)",
                "    plt.plot(fpr, tpr, linestyle = '--', color = colors[i], label = f'{classes[i]} vs Rest')",
                "# ROC curve for tpr = fpr",
                "plt.plot([0, 1], [0, 1], 'k--', label = 'Classifier')",
                "plt.title('Multiclass ROC curve')",
                "plt.xlabel('False positive Rate')",
                "plt.ylabel('True positive rate')",
                "plt.legend()",
                "plt.show()"
            ]
        },
        {
            "name" : "Grid search CV (classification)",
            "code" : [
                "from sklearn.model_selection import GridSearchCV",
                "",
                "parameters = {",
                "    'max_depth': [4, 8, 16, 32]",
                "}",
                "",
                "clf = RandomForestClassifier(random_state = 42)",
                "clf.fit(x_train, y_train)",
                "gs = GridSearchCV(estimator = clf, cv = 5, param_grid = parameters, n_jobs = -1, verbose = 4)",
                "gs.fit(x_train, y_train)",
                "print(f'Best parameters: {gs.best_params_}')",
                "clf.set_params(**gs.best_params_)",
                "y_pred = clf.predict(x_test)",
                "print(classification_report(y_test, y_pred))",
                "show_confusion_matrix(y_test, y_pred)"
            ]
        },
        {
            "name" : "Grid search CV (regression)",
            "code" : [
                "from sklearn.model_selection import GridSearchCV",
                "from sklearn.metrics import r2_score",
                "from sklearn.metrics import mean_absolute_error",
                "from sklearn.metrics import mean_squared_error",
                "",
                "parameters = {",
                "    'max_depth': [4, 8, 16, 32]",
                "}",
                "",
                "reg = RandomForestRegressor(random_state = 42)",
                "reg.fit(x_train, y_train)",
                "gs = GridSearchCV(estimator = reg, scoring = 'r2', cv = 5, param_grid = parameters, n_jobs = -1, verbose = 4)     ",
                "gs.fit(x_train, y_train)",
                "print(f'Best parameters: {gs.best_params_}')",
                "reg.set_params(**gs.best_params_)",
                "y_pred = reg.predict(x_test)",
                "print(f'R2 score: {r2_score(y_test, y_pred)}')",
                "print(f'MAE error: {mean_absolute_error(y_test, y_pred)}')",
                "print(f'MSE error: {mean_squared_error(y_test, y_pred)}')",
                "print(f'RMSE error: {np.sqrt(mean_squared_error(y_test, y_pred))}')"
            ]
        },
        {
            "name" : "Scale training and testing sets for CV",
            "code" : [
                "from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit",
                "",
                "def scale_train_and_test(train, test, columns_to_scale):",
                "    if train.shape[-1] != test.shape[-1]:",
                "        raise Exception('Train and test dataframes are incompatible')",
                "    scaler = StandardScaler()",
                "    train_scaled_columns = scaler.fit_transform(train[columns_to_scale])",
                "    test_scaled_columns = scaler.transform(test[columns_to_scale])",
                "    columns = list(train.columns)",
                "    for column in columns_to_scale:",
                "        columns.remove(column)",
                "    train_scaled = pd.DataFrame(",
                "        train.drop(columns = columns_to_scale),",
                "        index = train.index,",
                "        columns = columns)",
                "    for i, column in enumerate(columns_to_scale):",
                "        train_scaled[column] = train_scaled_columns.T[i]",
                "    test_scaled = pd.DataFrame(",
                "        test.drop(columns = columns_to_scale),",
                "        index = test.index,",
                "        columns = columns)",
                "    for i, column in enumerate(columns_to_scale):",
                "        test_scaled[column] = test_scaled_columns.T[i]",
                "    return train_scaled, test_scaled, scaler",
                "",
                "fold_num = 1",
                "fold = KFold(n_splits = 5)",
                "for train_range, test_range in fold.split(df):",
                "    train = df.iloc[train_range]",
                "    test = df.iloc[test_range]",
                "    columns_to_scale = ['columns_to_scale', 'target']",
                "    train_scaled, test_scaled, scaler = scale_train_and_test(train, test, columns_to_scale)",
                "    y_train = train_scaled['target']",
                "    y_test = test_scaled['target']",
                "    plt.figure(figsize = (10, 4))",
                "    plt.title(f'Fold #{fold_num}')",
                "    plt.plot(y_train)",
                "    plt.plot(y_test)",
                "    plt.show()",
                "    fold_num += 1"
            ]
        },
        {
            "name" : "Cross-validation using ML classifiers",
            "code" : [
                "from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit",
                "from sklearn.metrics import accuracy_score as accuracy, precision_score as precision",
                "from sklearn.linear_model import LogisticRegression",
                "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis",
                "from sklearn.neighbors import KNeighborsClassifier",
                "from sklearn.naive_bayes import GaussianNB",
                "from sklearn.tree import DecisionTreeClassifier",
                "from sklearn.ensemble import RandomForestClassifier",
                "from sklearn.ensemble import ExtraTreesClassifier",
                "from sklearn.ensemble import BaggingClassifier",
                "from sklearn.ensemble import AdaBoostClassifier",
                "from sklearn.ensemble import HistGradientBoostingClassifier",
                "from xgboost import XGBClassifier",
                "from catboost import CatBoostClassifier",
                "",
                "# Don't forget about the class labels!",
                "# labels = ['some', 'labels']",
                "# encoder = LabelEncoder()",
                "# encoder.fit(labels)",
                "",
                "models = []",
                "models.append(('LgR', LogisticRegression(random_state = 42, n_jobs = -1)))",
                "models.append(('LDA', LinearDiscriminantAnalysis()))",
                "models.append(('KNN', KNeighborsClassifier(n_neighbors = 20, n_jobs = -1)))",
                "models.append(('NB', GaussianNB()))",
                "models.append(('CART', DecisionTreeClassifier(random_state = 42)))",
                "models.append(('RF', RandomForestClassifier(n_jobs = -1, random_state = 42)))",
                "models.append(('XT', ExtraTreesClassifier(n_jobs = -1, random_state = 42)))",
                "models.append(('BG', BaggingClassifier(n_jobs = -1, random_state = 42)))",
                "models.append(('ADA', AdaBoostClassifier(random_state = 42)))",
                "models.append(('HG', HistGradientBoostingClassifier(random_state = 42)))",
                "models.append(('XGB', XGBClassifier(tree_method = 'gpu_hist', verbosity = 0, random_state = 42)))",
                "models.append(('CAT', CatBoostClassifier(",
                "    random_seed = 42,",
                "    loss_function = 'CrossEntropy',",
                "    eval_metric = 'Precision',",
                "    od_type = 'Iter',",
                "    od_wait = 20,",
                "    task_type = 'GPU',",
                "    logging_level = 'Silent')))",
                "results = {",
                "    'accuracy' : [],",
                "    'precision' : []",
                "}",
                "",
                "names = []",
                "for name, model in models:",
                "    names.append(name)",
                "    print('----------------')",
                "    print(f'{name} model...')",
                "    model_results = {",
                "        'accuracy' : [],",
                "        'precision' : []",
                "    }",
                "    fold_num = 1",
                "    fold = StratifiedKFold(n_splits = 5)",
                "    for train_range, test_range in fold.split(df):",
                "        print(f'Fold #{fold_num}...')",
                "        train = df.iloc[train_range]",
                "        test = df.iloc[test_range]",
                "        target_column = 'target'",
                "        columns_to_scale = ['columns_to_scale']",
                "        train_scaled, test_scaled, _ = scale_train_and_test(train, test, columns_to_scale)",
                "        x_train = train_scaled.drop(columns = target_column)",
                "        y_train = train_scaled[target_column]",
                "        x_test = test_scaled.drop(columns = target_column)",
                "        y_test = test_scaled[target_column]",
                "        model.fit(x_train, y_train)",
                "        y_pred = model.predict(x_test)",
                "        model_results['accuracy'].append(accuracy(y_test, y_pred))",
                "        model_results['precision'].append(precision(y_test, y_pred))",
                "        print(classification_report(",
                "            encoder.inverse_transform(y_test),",
                "            encoder.inverse_transform(y_pred)))",
                "        show_confusion_matrix(y_test, y_pred, labels)",
                "        fold_num += 1",
                "    for model_result_key, model_result_value in model_results.items():",
                "        results[model_result_key].append(model_result_value)",
                "    print('Done!')",
                "    print()",
                "for result_key, result_value in results.items():",
                "    plt.title(f'{result_key.capitalize()} scoring')",
                "    plt.boxplot(result_value, labels = names)",
                "    plt.show()"
            ]
        },
        {
            "name" : "Cross-validation using ML regressors",
            "code" : [
                "from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit",
                "from sklearn.metrics import r2_score as r2, mean_squared_error as mse",
                "from sklearn.metrics import mean_absolute_error as mae, mean_absolute_percentage_error as mape",
                "from sklearn.linear_model import LinearRegression",
                "from sklearn.neighbors import KNeighborsRegressor",
                "from sklearn.tree import DecisionTreeRegressor",
                "from sklearn.ensemble import RandomForestRegressor",
                "from sklearn.ensemble import ExtraTreesRegressor",
                "from sklearn.ensemble import BaggingRegressor",
                "from sklearn.ensemble import AdaBoostRegressor",
                "from sklearn.ensemble import HistGradientBoostingRegressor",
                "from xgboost import XGBRegressor",
                "from catboost import CatBoostRegressor",
                "",
                "models = []",
                "models.append(('LR', LinearRegression(n_jobs = -1)))",
                "models.append(('KNN', KNeighborsRegressor(n_neighbors = 20, n_jobs = -1)))",
                "models.append(('CART', DecisionTreeRegressor(random_state = 42)))",
                "models.append(('RF', RandomForestRegressor(n_jobs = -1, random_state = 42)))",
                "models.append(('XT', ExtraTreesRegressor(n_jobs = -1, random_state = 42)))",
                "models.append(('BG', BaggingRegressor(n_jobs = -1, random_state = 42)))",
                "models.append(('ADA', AdaBoostRegressor(random_state = 42)))",
                "models.append(('HG', HistGradientBoostingRegressor(random_state = 42)))",
                "models.append(('XGB', XGBRegressor(tree_method = 'gpu_hist', verbosity = 0, random_state = 42)))",
                "models.append(('CAT', CatBoostRegressor(",
                "    random_seed = 42,",
                "    loss_function = 'RMSE',",
                "    eval_metric = 'MAE',",
                "    od_type = 'Iter',",
                "    od_wait = 20,",
                "    task_type = 'GPU',",
                "    logging_level = 'Silent')))",
                "results = {",
                "    'r2' : [],",
                "    'mse' : [],",
                "    'rmse' : [],",
                "    'mae' : [],",
                "    'mape' : []",
                "}",
                "",
                "names = []",
                "for name, model in models:",
                "    names.append(name)",
                "    print('----------------')",
                "    print(f'{name} model...')",
                "    model_results = {",
                "        'r2' : [],",
                "        'mse' : [],",
                "        'rmse' : [],",
                "        'mae' : [],",
                "        'mape' : []",
                "    }",
                "    fold_num = 1",
                "    fold = KFold(n_splits = 5)",
                "    for train_range, test_range in fold.split(df):",
                "        print(f'Fold #{fold_num}...')",
                "        train = df.iloc[train_range]",
                "        test = df.iloc[test_range]",
                "        target_column = 'target'",
                "        columns_to_scale = ['columns_to_scale', 'target']",
                "        train_scaled, test_scaled, _ = scale_train_and_test(train, test, columns_to_scale)",
                "        x_train = train_scaled.drop(columns = target_column)",
                "        y_train = train_scaled[target_column]",
                "        x_test = test_scaled.drop(columns = target_column)",
                "        y_test = test_scaled[target_column]",
                "        model.fit(x_train, y_train)",
                "        y_pred = model.predict(x_test)",
                "        model_results['r2'].append(r2(y_test, y_pred))",
                "        model_results['mse'].append(mse(y_test, y_pred))",
                "        model_results['rmse'].append(np.sqrt(mse(y_test, y_pred)))",
                "        model_results['mae'].append(mae(y_test, y_pred))",
                "        model_results['mape'].append(mape(y_test, y_pred))",
                "        show_regression_chart(y_test, y_pred, sort = True)",
                "        fold_num += 1",
                "    for model_result_key, model_result_value in model_results.items():",
                "        results[model_result_key].append(model_result_value)",
                "    print('Done!')",
                "    print()",
                "for result_key, result_value in results.items():",
                "    plt.title(f'{result_key.upper()} scoring')",
                "    plt.boxplot(result_value, labels = names)",
                "    plt.show()"
            ]
        },
        {
            "name" : "Nested array example",
            "code" : [
                "nested_list = [",
                "    [",
                "        [",
                "            [1.0, 6.0, 7.0, 2.0],",
                "            [2.0, 16.0, 99.0, 23.0],",
                "            [3.0, 3.0, 1.0, 0.0]",
                "        ],",
                "        [",
                "            [5.0, 8.0, 7.0, 11.0],",
                "            [6.0, 16.0, 69.0, 453.0],",
                "            [7.0, 3.0, 7.0, 22.0]",
                "        ]",
                "    ],",
                "    [",
                "        [",
                "            [13.0, 8.0, 7.0, 11.0],",
                "            [14.0, 16.0, 69.0, 453.0],",
                "            [16.0, 3.0, 7.0, 32.0]",
                "        ],",
                "        [",
                "            [17.0, 8.0, 7.0, 11.0],",
                "            [19.0, 16.0, 69.0, 453.0],",
                "            [20.0, 3.0, 7.0, 220.0]",
                "        ]",
                "    ]",
                "]",
                "",
                "nested_array = np.array(nested_list)",
                "print(nested_array)",
                "print(nested_array.shape)"
            ]
        },
        {
            "name" : "MyMinMaxScaler",
            "code" : [
                "from sklearn.preprocessing import MinMaxScaler",
                "",
                "class MyMinMaxScaler(MinMaxScaler):",
                "    def fit_transform(self, X, y = None):",
                "        x = np.reshape(X, (-1, 1))",
                "        return np.reshape(super().fit_transform(x, y = y), newshape = X.shape)"
            ]
        },
        {
            "name" : "(NumPy) Random images (or random n-dim arrays)",
            "code" : [
                "random_color_images = (np.random.random((amount, width, height, channels)) * 255).astype('uint8')",
                "black_images = np.zeros((amount, width, height, channels)).astype('uint8')"
            ]
        },
        {
            "name" : "Class weights",
            "code" : [
                "from sklearn.utils.class_weight import compute_class_weight",
                "",
                "weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(y), y = y)",
                "weights"
            ]
        },
        {
            "name" : "PCA (Principal component analysis)",
            "code" : [
                "from sklearn.decomposition import PCA",
                "",
                "pca = PCA(n_components = 5)",
                "pca_x = pca.fit_transform(x_scaled)",
                "",
                "per_var = np.round(pca.explained_variance_ratio_ * 100.0, decimals = 4)",
                "labels = [str(i) for i in range(1, len(per_var) + 1)]",
                "print(f'Explained varience: {np.sum(per_var)}')",
                "plt.figure(figsize = (10, 6))",
                "plt.bar(x = range(1, len(per_var) + 1), height = per_var, tick_label = labels)",
                "plt.plot(range(1, len(per_var) + 1), np.cumsum(per_var), c = 'red', label = 'Cumulative explained variance')",
                "plt.legend(loc = 'upper left')",
                "plt.ylabel('Percentage of explained variance')",
                "plt.xlabel('Principal component')",
                "plt.title('Scree plot')",
                "plt.show()"
            ]
        },
        {
            "name" : "Plot decision regions (mlxtend)",
            "code" : [
                "from sklearn.ensemble import RandomForestClassifier",
                "from sklearn.linear_model import LogisticRegression",
                "from mlxtend.plotting import plot_decision_regions",
                "",
                "selected_x_train = x_train[:, [0, 1]]",
                "#clf = LogisticRegression(random_state = 42)",
                "clf = RandomForestClassifier(random_state = 42)",
                "clf.fit(selected_x_train, y_train)",
                "",
                "plot_decision_regions(selected_x_train, y_train, clf)",
                "plt.show()"
            ]
        },
        {
            "name" : "(Dataprep) Automatic EDA",
            "code" : [
                "from dataprep.eda import plot, plot_correlation, create_report, plot_missing",
                "import pandas as pd",
                "",
                "data = pd.read_csv('...')",
                "# Plot columns",
                "plot(data)",
                "# Plot and explore one of the columns",
                "plot(data, 'column')",
                "# Correlations plots",
                "plot_correlation(data)",
                "# Explore missing data",
                "plot_missing(data)",
                "# Create report based on the dataset",
                "create_report(data)"
            ]
        },
        {
            "name" : "My synthetic dataset",
            "code" : [
                "def my_random():",
                "    return np.round(np.random.random() * 100 + 0.5)",
                "",
                "def make_synthetic_dataset(rows_count, columns_count):",
                "    if rows_count <= 0 or columns_count <= 0:",
                "        raise Exception('Invalid rows/columns count')",
                "    xs = []",
                "    for i in range(columns_count):",
                "        xs.append(np.random.normal(loc = my_random(), scale = my_random(), size = rows_count))",
                "    y = np.random.normal(loc = my_random(), scale = my_random(), size = rows_count)",
                "    xs.append(y)",
                "    xs = np.array(xs)",
                "    xs = np.transpose(xs)",
                "    return pd.DataFrame(xs, columns = [f'x{i + 1}' for i in range(xs.shape[1] - 1)] + ['y'])"
            ]
        },
        {
            "name" : "Feature importance based on mean decrease in impurity",
            "code" : [
                "importances = forest.feature_importances_",
                "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis = 0)",
                "forest_importances = pd.Series(importances, index = list(x.columns))",
                "fig, ax = plt.subplots()",
                "forest_importances.plot.bar(yerr = std, ax = ax)",
                "ax.set_title('Feature importances using MDI')",
                "ax.set_ylabel('Mean decrease in impurity')",
                "fig.tight_layout()"
            ]
        },
        {
            "name" : "Feature importance based on feature permutation",
            "code" : [
                "from sklearn.inspection import permutation_importance",
                "",
                "result = permutation_importance(",
                "    forest,",
                "    x_test,",
                "    y_test,",
                "    n_repeats = 10,",
                "    random_state = 42,",
                "    n_jobs = -1)",
                "forest_importances = pd.Series(result.importances_mean, index = list(x.columns))",
                "fig, ax = plt.subplots()",
                "forest_importances.plot.bar(yerr = result.importances_std, ax = ax)",
                "ax.set_title('Feature importances using permutation on full model')",
                "ax.set_ylabel('Mean accuracy decrease')",
                "fig.tight_layout()",
                "plt.show()"
            ]
        },
        {
            "name" : "Draw points on the world's map",
            "code" : [
                "from shapely.geometry import Point",
                "import geopandas as gpd",
                "from geopandas import GeoDataFrame",
                "",
                "geometry = [Point(xy) for xy in zip(df['geo_lon'], df['geo_lat'])]",
                "gdf = GeoDataFrame(df, geometry = geometry)   ",
                "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))",
                "gdf.plot(",
                "    ax = world.plot(figsize = (20, 10)),",
                "    marker = '+',",
                "    color = 'red',",
                "    markersize = 15)"
            ]
        },
        {
            "name" : "Draw points on county's map",
            "code" : [
                "from shapely.geometry import Point, Polygon",
                "import geopandas as gpd",
                "from geopandas import GeoDataFrame",
                "",
                "df = pd.read_csv('...')",
                "# https://gadm.org/download_country.html",
                "reg_map = gpd.read_file('example-map/gadm41_RUS_1.shp')",
                "geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]",
                "crs = {'init': 'epsg:4326'}",
                "gdf = GeoDataFrame(df, crs = crs, geometry = geometry)",
                "fig, ax = plt.subplots(figsize = (40, 30))",
                "reg_map.plot(ax = ax)",
                "gdf.plot(",
                "    ax = ax,",
                "    markersize = 10,",
                "    color = 'red',",
                "    marker = '+'',",
                "    label = 'Apartments')",
                "plt.legend(",
                "    prop = {'size': 25},",
                "    loc = 'center left',",
                "    fancybox = True,",
                "    framealpha = 1,",
                "    shadow = False,",
                "    borderpad = 1,",
                "    labelcolor = 'black',",
                "    facecolor = 'lightseagreen',",
                "    edgecolor = 'lightseagreen',",
                "    markerscale = 11)",
                "plt.show()"
            ]
        },
        {
            "name" : "Autocorrelation, partial autocorr., seasonal decomposition",
            "code" : [
                "from statsmodels.graphics.tsaplots import plot_acf",
                "from statsmodels.graphics.tsaplots import plot_pacf",
                "from pandas.plotting import autocorrelation_plot",
                "from statsmodels.tsa.seasonal import seasonal_decompose",
                "",
                "def make_acf_plots(df):",
                "    for column in df:",
                "        if df[column].dtypes == 'object':",
                "            continue",
                "        plot_acf(",
                "            df[column].to_numpy(),",
                "            title = column,",
                "            missing = 'drop')",
                "        plt.show()",
                "",
                "def make_pacf_plots(df):",
                "    for column in df:",
                "        if df[column].dtypes == 'object':",
                "            continue",
                "        plot_pacf(",
                "            df[column].to_numpy(),",
                "            title = column)",
                "        plt.show()        ",
                "",
                "def make_autocorrelation_plots(df):",
                "    for column in df:",
                "        if df[column].dtypes == 'object':",
                "            continue",
                "        plt.title(column)",
                "        autocorrelation_plot(df[column]) ",
                "        plt.show()",
                "",
                "def make_seasonal_decomposition_plots(df):",
                "    for column in df:",
                "        if df[column].dtypes == 'object':",
                "            continue",
                "        seasonal_decompose(df[column], period = 365, model ='additive').plot()",
                "        # seasonal_decompose(df[column], period = 365, model ='multiplicative').plot()",
                "        plt.show()"
            ]
        },
        {
            "name" : "Timestamp preparation for time series dataset",
            "code" : [
                "df.drop_duplicates(subset = 'Datetime', inplace = True)",
                "df = df.set_index('Datetime')",
                "df.index = pd.to_datetime(df.index)",
                "df = df.asfreq('1H')",
                "df.sort_index(inplace = True)",
                "df.rename(columns = {'PJMW_MW': 'Target'}, inplace = True)",
                "df.fillna({'Target': df['Target'].mean()}, inplace = True)",
                "df"
            ]
        }
    ]
}
