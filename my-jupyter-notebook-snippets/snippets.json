{
    "snippets" : [
        {
            "name" : "Notebook expressions output",
            "code" : [
                "from IPython.core.interactiveshell import InteractiveShell",
                "",
                "InteractiveShell.ast_node_interactivity = 'all' # Show all results of expression from the cell",
                "InteractiveShell.ast_node_interactivity = 'last_expr' # Revert back to show only results of the last expression"
            ]
        },
        {
            "name" : "Warning filters",
            "code" : [
                "import warnings",
                "",
                "warnings.filterwarnings('ignore', category = FutureWarning)",
                "warnings.filterwarnings('ignore', category = UserWarning)"
            ]
        },
        {
            "name" : "Garbage collector",
            "code" : [
                "import gc",
                "",
                "gc.collect()"
            ]
        },
        {
            "name" : "Google Drive disk mount",
            "code" : [
                "from google.colab import drive",
                "",
                "drive.mount('/content/drive')",
                "drive_path = 'drive/MyDrive'"
            ]
        },
        {
            "name" : "Compress notebook",
            "code" : [
                "import os",
                "from ipynbcompress import compress",
                "def compress_notebook(file_in, file_out):",
                "",
                "    old_size = os.stat(file_in).st_size / 1024 ** 2",
                "    new_size = compress(file_in, output_filename = file_out, img_format = 'jpeg')/ 1024 ** 2",
                "    print(f'Notebook size after optimization is: {new_size:.2f} MB')",
                "    decrease_mem_percent = 100 * (old_size - new_size) / old_size",
                "    print(f'Notebook size has decreased by {decrease_mem_percent:.1f}%')"
            ]
        },
        {
            "name" : "Reduce memory usage for the dataset",
            "code" : [
                "import numpy as np",
                "",
                "def reduce_mem_usage(df_orig):",
                "    df = df_orig.copy()",
                "    start_mem = df.memory_usage().sum() / 1024**2",
                "    print(f'Memory usage of dataframe is {start_mem:.2f} MB')",
                "    for col in df.columns:",
                "        col_type = df[col].dtype",
                "        if col_type != object:",
                "            c_min = df[col].min()",
                "            c_max = df[col].max()",
                "            if str(col_type)[:3] == 'int':",
                "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:",
                "                    df[col] = df[col].astype(np.int8)",
                "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:",
                "                    df[col] = df[col].astype(np.int16)",
                "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:",
                "                    df[col] = df[col].astype(np.int32)",
                "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:",
                "                    df[col] = df[col].astype(np.int64)  ",
                "            else:",
                "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:",
                "                    df[col] = df[col].astype(np.float16)",
                "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:",
                "                    df[col] = df[col].astype(np.float32)",
                "                else:",
                "                    df[col] = df[col].astype(np.float64)",
                "        else:",
                "            df[col] = df[col].astype('category')",
                "    end_mem = df.memory_usage().sum() / 1024**2",
                "    print(f'Memory usage after optimization is: {end_mem:.2f} MB')",
                "    decrease_mem_percent = 100 * (start_mem - end_mem) / start_mem",
                "    print(f'Decreased by {decrease_mem_percent:.1f}%')",
                "    return df"
            ]
        },
        {
            "name" : "Images quality tweaks",
            "code" : [
                "# image format",
                "%config InlineBackend.figure_format = 'jpg'",
                "# dpi tweak (default - 100)",
                "matplotlib.rcParams['figure.dpi'] = 100",
                "# plot size tweak",
                "matplotlib.pyplot.rcParams['figure.figsize'] = (10, 3) # makes figures larger"
            ]
        },
        {
            "name" : "Imports",
            "code" : [
                "import numpy as np",
                "import pandas as pd",
                "pd.set_option('display.max_rows', 50)",
                "pd.set_option('display.max_columns', 10)",
                "import matplotlib.pyplot as plt",
                "import seaborn as sb",
                "import missingno as msno",
                "import random as rd"
            ]
        },
        {
            "name" : "ML",
            "code" : [
                "from sklearn.metrics import confusion_matrix, classification_report",
                "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder",
                "from sklearn.model_selection import train_test_split",
                "# Classification",
                "from sklearn.model_selection import GridSearchCV",
                "from sklearn.neighbors import KNeighborsClassifier",
                "from sklearn.tree import DecisionTreeClassifier",
                "from sklearn.linear_model import LogisticRegression",
                "from sklearn.linear_model import SGDClassifier",
                "from sklearn.svm import LinearSVC",
                "from sklearn.svm import SVC",
                "from sklearn.ensemble import RandomForestClassifier",
                "from sklearn.ensemble import ExtraTreesClassifier",
                "from sklearn.ensemble import BaggingClassifier",
                "from sklearn.ensemble import AdaBoostClassifier",
                "from sklearn.ensemble import GradientBoostingClassifier",
                "from sklearn.ensemble import HistGradientBoostingClassifier",
                "from sklearn.ensemble import VotingClassifier",
                "from sklearn.naive_bayes import GaussianNB",
                "from catboost import CatBoostClassifier",
                "from xgboost import XGBClassifier",
                "# Regression",
                "from sklearn.neighbors import KNeighborsRegressor",
                "from sklearn.linear_model import LinearRegression",
                "from sklearn.tree import DecisionTreeRegressor",
                "from sklearn.ensemble import RandomForestRegressor",
                "from sklearn.ensemble import ExtraTreesRegressor",
                "from sklearn.ensemble import BaggingRegressor",
                "from sklearn.ensemble import AdaBoostRegressor",
                "from sklearn.ensemble import HistGradientBoostingRegressor",
                "from catboost import CatBoostRegressor",
                "from xgboost import XGBRegressor",
                "from sklearn.model_selection import GridSearchCV",
                "from sklearn.metrics import r2_score",
                "from sklearn.metrics import mean_absolute_error",
                "from sklearn.metrics import mean_squared_error"
            ]
        },
        {
            "name" : "TF & Keras",
            "code" : [
                "import tensorflow as tf",
                "from tensorflow import keras",
                "from tensorflow.keras.utils import to_categorical",
                "from keras.callbacks import EarlyStopping, ModelCheckpoint",
                "from keras.models import Sequential",
                "from keras.layers import Dense, Flatten, Dropout, Conv2D, BatchNormalization, ReLU",
                "from keras.layers import GRU, LSTM, Bidirectional",
                "from keras.layers import MaxPooling2D, AveragePooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D",
                "# Image preprocessing",
                "from keras.layers import Rescaling, Resizing, CenterCrop",
                "# Image augmentation",
                "from keras.preprocessing.image import ImageDataGenerator",
                "from keras.layers import RandomCrop, RandomFlip, RandomTranslation, RandomRotation, RandomZoom, RandomContrast",
                "",
                "# List of devices",
                "tf.config.list_physical_devices()",
                "",
                "# Plot model function",
                "# keras.utils.plot_model(model, show_shapes = True)",
                "",
                "# Model loading function",
                "# model = keras.models.load_model('model.keras', compile = False)",
                "# model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])",
                "# model.summary()",
                "",
                "# Show training history",
                "def show_training_history(history, metrics, metric_names, figure_size = (7, 6), last_epoch = None):",
                "    if len(metrics) == 1 and len(metric_names) == 1:",
                "        fig, ax = plt.subplots(figsize = figure_size)",
                "        metric = metrics[0]",
                "        metric_name = metric_names[0]",
                "        ax.plot(history[metric], color = 'b', label = f'Training {metric_name}')",
                "        if f'val_{metric}' in history:",
                "            ax.plot(history[f'val_{metric}'], color = 'r', label = f'Validation {metric_name}')",
                "        if last_epoch is not None:",
                "            ax.axvline(x = last_epoch, color = 'g', label = 'Start of fine tuning')",
                "        legend = ax.legend(loc = 'best', shadow = True)",
                "        plt.show()",
                "    elif len(metrics) > 1 and len(metrics) == len(metric_names):",
                "        fig, ax = plt.subplots(len(metrics), 1, figsize = figure_size)",
                "        for i in range(len(metrics)):",
                "            metric = metrics[i]",
                "            metric_name = metric_names[i]",
                "            ax[i].plot(history[metric], color = 'b', label = f'Training {metric_name}')",
                "            if f'val_{metric}' in history:",
                "                ax[i].plot(history[f'val_{metric}'], color = 'r', label = f'Validation {metric_name}', axes = ax[i])",
                "            if last_epoch is not None:",
                "                ax[i].axvline(x = last_epoch, color = 'g', label = 'Start of fine tuning')",
                "            legend = ax[i].legend(loc = 'best', shadow = True)",
                "        plt.show()",
                "    else:",
                "        raise Exception('Invalid metrics/metric names amount')"
            ]
        },
        {
            "name" : "Testing datasets",
            "code" : [
                "from sklearn.preprocessing import LabelEncoder",
                "from sklearn.datasets import fetch_openml",
                "from tensorflow.keras.datasets import mnist",
                "from extra_keras_datasets import emnist",
                "from keras.datasets import cifar10",
                "",
                "def is_point_inside_circle(x, y, center_x, center_y, radius):",
                "    return (x - center_x) ** 2 + (y - center_y) ** 2 < radius ** 2",
                "",
                "def generate_donut(count, center_x, center_y, big_radius, small_radius):",
                "    if (big_radius == small_radius):",
                "        small_radius -= 0.01",
                "    if (small_radius > big_radius):",
                "        t = small_radius",
                "        small_radius = big_radius",
                "        big_radius = t",
                "    generated_points = []",
                "    while (len(generated_points) < count):",
                "        values = np.random.rand(1, 2) * 2 * big_radius - big_radius",
                "        x = values[0][0] + center_x",
                "        y = values[0][1] + center_y",
                "        if (is_point_inside_circle(x, y, center_x, center_y, big_radius)",
                "            and not is_point_inside_circle(x, y, center_x, center_y, small_radius)):",
                "            generated_points.append([x, y])",
                "    return np.array(generated_points)",
                "",
                "def generate_donut_dataset(class_count):",
                "    donut_0 = pd.DataFrame(generate_donut(class_count, 0, 0, 1, 0.5), columns = ['x', 'y'])",
                "    donut_0['class'] = 0",
                "    donut_1 = pd.DataFrame(generate_donut(class_count, 0, 0, 2, 1.25), columns = ['x', 'y'])",
                "    donut_1['class'] = 1",
                "    donut_2 = pd.DataFrame(generate_donut(class_count, 0, 0, 2.25, 2.75), columns = ['x', 'y'])",
                "    donut_2['class'] = 2",
                "    return pd.concat([donut_0, donut_1, donut_2], ignore_index = True)",
                "",
                "def xor(x, y):",
                "    return bool(x > 0) ^ bool(y > 0)",
                "",
                "def get_xor_dataset(size):",
                "    values = np.random.rand(size, 2) * 2 - 1",
                "    df = pd.DataFrame(values, columns = ['x', 'y'])",
                "    df['class'] = df.apply(lambda row: int(xor(row['x'], row['y'])), axis = 1)",
                "    return df",
                "",
                "def my_random():",
                "    return np.round(np.random.random() * 100 + 0.5)",
                "",
                "def make_synthetic_dataset(rows_count, columns_count):",
                "    if rows_count <= 0 or columns_count <= 0:",
                "        raise Exception('Invalid rows/columns count')",
                "    xs = []",
                "    for i in range(columns_count):",
                "        xs.append(np.random.normal(loc = my_random(), scale = my_random(), size = rows_count))",
                "    y = np.random.normal(loc = my_random(), scale = my_random(), size = rows_count)",
                "    xs.append(y)",
                "    xs = np.array(xs)",
                "    xs = np.transpose(xs)",
                "    return pd.DataFrame(xs, columns = [f'x{i + 1}' for i in range(xs.shape[1] - 1)] + ['y'])",
                "",
                "def get_random_color_images(width, height, channels, amount):",
                "    return (np.random.random((amount, width, height, channels)) * 255).astype('uint8')",
                "",
                "def get_black_images(width, height, channels, amount):",
                "    return np.zeros((amount, width, height, channels)).astype('uint8')",
                "",
                "mnist_784 = fetch_openml('mnist_784', version = 1, cache = True, parser = 'auto')",
                "mnist_784_x = mnist_784['data']",
                "mnist_784_y = mnist_784['target']",
                "",
                "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()",
                "",
                "(emnist_x_train, emnist_y_train), (emnist_x_test, emnist_y_test) = emnist.load_data(type = 'balanced')",
                "# Labels for the balanced dataset",
                "# https://arxiv.org/pdf/1702.05373v1.pdf",
                "labels = [str(i) for i in range(0, 10)]",
                "labels += ['a', 'b', 'd', 'e', 'f', 'g', 'h', 'n', 'q', 'r', 't']",
                "labels += [chr(i) for i in range(65, 91)]",
                "",
                "(cifar10_x_train, cifar10_y_train), (cifar10_x_test, cifar10_y_test) = cifar10.load_data()",
                "labels = ['airplane', 'automobile', 'bird',",
                "          'cat', 'deer', 'dog', 'frog',",
                "          'horse', 'ship', 'truck']",
                "",
                "# labels = 'y'",
                "encoder = LabelEncoder()",
                "encoder.fit(labels)",
                "print(encoder.inverse_transform([0]))",
                "print(encoder.classes_)"
            ]
        },
        {
            "name" : "Plot functions (classification)",
            "code" : [
                "def show_image_dataset(x, y, encoder = None):",
                "    plt.subplots(4, 5, figsize = (8, 5))",
                "    plt.tight_layout()",
                "    for i in range(0, 20):",
                "        n = np.random.randint(0, x.shape[0])",
                "        plt.subplot(4, 5, i + 1)",
                "        plt.axis('off')",
                "        if encoder is None:",
                "            plt.gca().set_title(f'{y[n]}')",
                "        else:",
                "            plt.gca().set_title(f'{encoder.inverse_transform([y[n]])[0]} ({y[n]})')",
                "        plt.imshow(x[n])",
                "    plt.show()",
                "",
                "def show_class_distribution(y, encoder = None):",
                "    ticks = [i for i in range(0, len(y.unique()))]",
                "    plt.figure(figsize = (10, 4))",
                "    plt.title('Class distribution')",
                "    if encoder is None:",
                "        sb.histplot(y, discrete = True)",
                "    else:",
                "        sb.histplot(encoder.inverse_transform(y), discrete = True)",
                "    plt.xticks(ticks)",
                "    plt.show()",
                "",
                "def show_confusion_matrix(labels_test, labels_predicted, figure_size = (10, 7), labels = None):",
                "    plt.figure(figsize = figure_size)",
                "    cm = confusion_matrix(labels_test, labels_predicted)",
                "    if labels is None:",
                "        sb.heatmap(",
                "            cm,",
                "            annot = True,",
                "            fmt = 'd',",
                "            cbar = False)",
                "    else:",
                "        sb.heatmap(",
                "            cm,",
                "            annot = True,",
                "            fmt = 'd',",
                "            cbar = False,",
                "            xticklabels = labels,",
                "            yticklabels = labels)",
                "    plt.xlabel('Predicted')",
                "    plt.ylabel('Truth')",
                "    plt.show()"
            ]
        },
        {
            "name" : "Plot functions (regression)",
            "code" : [
                "from scipy import stats as st",
                "import statsmodels.api as sm",
                "from pandas.plotting import autocorrelation_plot",
                "from statsmodels.graphics.tsaplots import plot_pacf",
                "from statsmodels.tsa.seasonal import seasonal_decompose",
                "",
                "def show_line_plot(data, title, figure_size = (10, 6)):",
                "    plt.figure(figsize = figure_size)",
                "    plt.plot(data)",
                "    plt.xticks(rotation = 45)",
                "    plt.title(title)",
                "    plt.show()",
                "",
                "def show_probability_plot(data):",
                "    st.probplot(data, dist = 'norm', plot = plt)",
                "    plt.show()",
                "",
                "def show_quantile_quantile_plot(data):",
                "    sm.qqplot(data, line = '45')",
                "    plt.show()",
                "",
                "def show_regression_chart(y, prediction, title, sort = False):",
                "    t = pd.DataFrame({",
                "        'prediction': prediction,",
                "        'y': y",
                "    })",
                "    if sort:",
                "        t.sort_values(by = ['y'], inplace = True)",
                "    plt.figure(figsize = (10, 6))",
                "    plt.plot(t['y'].to_numpy(), label = 'Test')",
                "    plt.plot(t['prediction'].to_numpy(), label = 'Prediction')",
                "    plt.legend()",
                "    plt.xticks(rotation = 45)",
                "    plt.title(title)",
                "    plt.show()",
                "",
                "def show_histogram(data, title):",
                "    fig, (ax_box, ax_hist) = plt.subplots(",
                "        2,",
                "        sharex = True,",
                "        gridspec_kw = {'height_ratios': (0.15, 0.85)}",
                "    ) ",
                "    sb.boxplot(x = data, ax = ax_box)",
                "    sb.histplot(x = data, ax = ax_hist, kde = True)",
                "    plt.title(title)",
                "    plt.show()",
                "",
                "def show_acf_plot(data, title):",
                "    autocorrelation_plot(data)",
                "    plt.title(title)",
                "    plt.show()",
                "",
                "def make_pacf_plots(df):",
                "    for column in df:",
                "        if df[column].dtypes == 'object':",
                "            continue",
                "        plot_pacf(",
                "            df[column].to_numpy(),",
                "            title = column,",
                "            lags = 50 # len(df[column]) / 2 - 1)",
                "        plt.show()",
                "",
                "def make_seasonal_decomposition_plots(df, period):",
                "    for column in df:",
                "        if df[column].dtypes == 'object':",
                "            continue",
                "        seasonal_decompose(df[column], period = period, model ='additive').plot()",
                "        # seasonal_decompose(df[column], period = period, model ='multiplicative').plot()",
                "        plt.show()"
            ]
        },
        {   "name" : "Plot functions (common)",
            "code" : [
                "def plot_dataset(df):",
                "    df.plot()",
                "    plt.show()",
                "",
                "def plot_dataset_histograms(df):",
                "    df.hist()",
                "    plt.show()",
                "",
                "def show_area_plot(df):",
                "    df.plot.area()",
                "    plt.show()",
                "",
                "def show_proportional_area_plot(df):",
                "    dfp = df.select_dtypes(exclude = ['object']).iloc[:, :].apply(lambda x: x / x.sum(), axis = 1)",
                "    dfp.plot.area()",
                "    plt.show()",
                "",
                "def show_correlation_map(df, figure_size = (15, 15)):",
                "    plt.figure(figsize = figure_size)",
                "    corr_matrix = df.select_dtypes(exclude = ['object']).corr()",
                "    lower = corr_matrix.where(np.tril(np.ones(corr_matrix.shape), k = -1).astype(np.bool_))",
                "    sb.heatmap(lower, annot = True, fmt = '.2f', cbar = False)",
                "    plt.show()",
                "",
                "def show_pair_plot(df, figure_size = (15, 15), target_name = None):",
                "    plt.figure(figsize = figure_size)",
                "    df_without_object_columns = df.select_dtypes(exclude = ['object'])",
                "    if target_name != None:",
                "        sb.pairplot(df_without_object_columns, corner = True, palette = 'tab10', hue = target_name)",
                "    else:",
                "        sb.pairplot(df_without_object_columns, corner = True)",
                "    plt.show()",
                "",
                "def show_cat_plot(df):",
                "    sb.catplot(df, kind = 'boxen', height = 8)",
                "    plt.show()",
                "",
                "def show_top_classes(df, column, num_of_top_classes = None):",
                "    top = df[column].value_counts()",
                "    if num_of_top_classes != None:",
                "        top = top[:num_of_top_classes]",
                "    plt.figure(figsize = (10, 7))",
                "    sb.barplot(x = top.index, y = top.values)",
                "    plt.xticks(rotation = 45)",
                "    plt.title('Top classes', color = 'blue')",
                "    plt.show()"
            ]
        },
        {
            "name" : "Count and remove outliers",
            "code" : [
                "from scipy import stats",
                "import numpy as np",
                "import pandas as pd",
                "",
                "def remove_outliers(df, column, lower = 0.01, upper = 0.99):",
                "    q_low = df[column].quantile(lower)",
                "    q_high = df[column].quantile(upper)",
                "    result = df[(df[column] < q_high) & (df[column] > q_low)]",
                "    return result",
                "",
                "def remove_outliers_z_score(df, column):",
                "    result = df.copy()",
                "    result['z_score'] = stats.zscore(df[column])",
                "    result = result.loc[result['z_score'].abs() <= 3]",
                "    return result",
                "",
                "def remove_outliers_iqr(df, column):",
                "    q1 = np.percentile(df[column], 25, method = 'linear')",
                "    q3 = np.percentile(df[column], 75, method = 'linear')",
                "    iqr = q3 - q1",
                "    lower = q1 - 1.5*iqr",
                "    upper = q3 + 1.5*iqr",
                "    upper_array = np.where(df[column] >= upper)[0]",
                "    lower_array = np.where(df[column] <= lower)[0]",
                "    result = df.copy().reset_index().drop(columns = 'date')",
                "    result.drop(index = upper_array, inplace = True)",
                "    result.drop(index = lower_array, inplace = True)",
                "    return result",
                "",
                "def count_outliers(df, columns):",
                "    outliers_count_result = {",
                "        'column_name': [],",
                "        'method_name': [],",
                "        'count': [],",
                "        'ratio': []",
                "    }",
                "    for column in columns:",
                "        outlier_dfs = []",
                "        outlier_dfs.append((remove_outliers(df, column, 0.01, 0.99), 'quantile_01_99')) ",
                "        outlier_dfs.append((remove_outliers(df, column, 0.05, 0.95), 'quantile_05_95'))",
                "        outlier_dfs.append((remove_outliers_z_score(df, column), 'z_score'))",
                "        outlier_dfs.append((remove_outliers_iqr(df, column), 'iqr'))",
                "        for outlier_df, name in outlier_dfs:",
                "            count = len(df) - len(outlier_df)",
                "            ratio = round(count * 100.0 / len(df), 2)",
                "            outliers_count_result['column_name'].append(column)",
                "            outliers_count_result['method_name'].append(name)",
                "            outliers_count_result['count'].append(count)",
                "            outliers_count_result['ratio'].append(ratio)",
                "    return pd.DataFrame(outliers_count_result)"
            ]
        },
        {
            "name" : "Pandas functions",
            "code" : [
                "def dataset_lookup(df):",
                "    print('Dataset info:')",
                "    print(df.info())",
                "    print()",
                "    print('Dataset description:')",
                "    print(df.describe())",
                "    print()",
                "    print('Amount of duplicates:')",
                "    duplicates_amount = df.duplicated().sum()",
                "    print(duplicates_amount)",
                "    if (duplicates_amount > 0):",
                "        print('Duplicates:')",
                "        print(df[df.duplicated(keep = False)])",
                "    print()",
                "    print('Amount of missing values:')",
                "    print(df.isna().sum(axis = 0))",
                "    print()",
                "    print('Unique values in each column:')",
                "    object_types_count = 0",
                "    for column in df:",
                "        if df[column].dtypes == 'object':",
                "            object_types_count += 1",
                "            print('-' * len(column))",
                "            print(f'{column}: {df[column].unique()}')",
                "    if object_types_count == 0:",
                "        print('-' * 10)",
                "        print('No object columns in this dataset.')",
                "",
                "def transpose_dataset(df):",
                "    return df.describe().transpose()",
                "",
                "def replace_bool_with_int(df):",
                "    return df.replace({False: 0, True: 1})",
                "",
                "def drop_rows_range(df, start_index, end_index):",
                "    df_dropped = df.drop(df.index[start_index:end_index])",
                "    return df_dropped",
                "",
                "def add_rows_to_dataset(df, dataframe_rows):",
                "    return pd.concat([df] + dataframe_rows, ignore_index = True)"
            ]
        },
        {
            "name" : "Encodings",
            "code" : [
                "from sklearn.preprocessing import LabelEncoder",
                "",
                "label_encoder = LabelEncoder()",
                "dataset_encoded = dataset.copy()",
                "dataset_encoded['column'] = label_encoder.fit_transform(dataset_encoded['column'])",
                "dataset_encoded",
                "",
                "dataset_encoded = pd.get_dummies(dataset, columns = ['column'])",
                "dataset_encoded.replace({False: 0, True: 1}, inplace = True)",
                "dataset_encoded"
            ]
        },
        {
            "name" : "ROC curve (2 classes)",
            "code" : [
                "from sklearn.metrics import roc_curve, roc_auc_score",
                "",
                "y_pred_proba = model.predict_proba(x_test)[:, 1]",
                "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)",
                "roc_auc = roc_auc_score(y_test, y_pred_proba)",
                "# Plot the ROC curve",
                "plt.plot(fpr, tpr, label = f'ROC curve (area = {roc_auc})')",
                "# ROC curve for tpr = fpr",
                "plt.plot([0, 1], [0, 1], 'k--', label = 'Classifier')",
                "plt.xlabel('False positive Rate')",
                "plt.ylabel('True positive Rate')",
                "plt.title('ROC curve')",
                "plt.legend(loc = 'lower right')",
                "plt.show()"
            ]
        },
        {
            "name" : "ROC curve (N classes)",
            "code" : [
                "from sklearn.metrics import roc_curve, roc_auc_score",
                "from sklearn.multiclass import OneVsRestClassifier",
                "",
                "oneVsRest = OneVsRestClassifier(SomeModel(random_state = 42))",
                "oneVsRest.fit(x_train, y_train)",
                "y_pred_proba = oneVsRest.predict_proba(x_test)",
                "roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class = 'ovr')",
                "print(f'ROC-AUC Score: {roc_auc}')",
                "colors = ['red', 'green', 'blue', 'yellow']",
                "classes = ['0', '1', '2', '3']",
                "for i in range(len(classes)): ",
                "    fpr, tpr, thresh = roc_curve(y_test, y_pred_proba[:, i], pos_label = i)",
                "    plt.plot(fpr, tpr, linestyle = '--', color = colors[i], label = f'{classes[i]} vs Rest')",
                "# ROC curve for tpr = fpr",
                "plt.plot([0, 1], [0, 1], 'k--', label = 'Classifier')",
                "plt.title('Multiclass ROC curve')",
                "plt.xlabel('False positive Rate')",
                "plt.ylabel('True positive rate')",
                "plt.legend()",
                "plt.show()"
            ]
        },
        {
            "name" : "PCA (Principal component analysis)",
            "code" : [
                "from sklearn.decomposition import PCA",
                "",
                "pca = PCA(n_components = 5)",
                "pca_x = pca.fit_transform(x_scaled)",
                "",
                "per_var = np.round(pca.explained_variance_ratio_ * 100.0, decimals = 4)",
                "labels = [str(i) for i in range(1, len(per_var) + 1)]",
                "print(f'Explained varience: {np.sum(per_var)}')",
                "plt.figure(figsize = (10, 6))",
                "plt.bar(x = range(1, len(per_var) + 1), height = per_var, tick_label = labels)",
                "plt.plot(range(1, len(per_var) + 1), np.cumsum(per_var), c = 'red', label = 'Cumulative explained variance')",
                "plt.legend(loc = 'upper left')",
                "plt.ylabel('Percentage of explained variance')",
                "plt.xlabel('Principal component')",
                "plt.title('Scree plot')",
                "plt.show()"
            ]
        },
        {
            "name" : "Plot decision regions (mlxtend)",
            "code" : [
                "from sklearn.ensemble import RandomForestClassifier",
                "from sklearn.linear_model import LogisticRegression",
                "from mlxtend.plotting import plot_decision_regions",
                "",
                "selected_x_train = x_train[:, [0, 1]]",
                "#clf = LogisticRegression(random_state = 42)",
                "clf = RandomForestClassifier(random_state = 42)",
                "clf.fit(selected_x_train, y_train)",
                "",
                "plot_decision_regions(selected_x_train, y_train, clf)",
                "plt.show()"
            ]
        },
        {
            "name" : "(Dataprep) Automatic EDA",
            "code" : [
                "from dataprep.eda import plot, plot_correlation, create_report, plot_missing",
                "import pandas as pd",
                "",
                "data = pd.read_csv('...')",
                "# Plot columns",
                "plot(data)",
                "# Plot and explore one of the columns",
                "plot(data, 'column')",
                "# Correlations plots",
                "plot_correlation(data)",
                "# Explore missing data",
                "plot_missing(data)",
                "# Create report based on the dataset",
                "create_report(data)"
            ]
        },
        {
            "name" : "Draw points on the world's map",
            "code" : [
                "from shapely.geometry import Point",
                "import geopandas as gpd",
                "from geopandas import GeoDataFrame",
                "",
                "geometry = [Point(xy) for xy in zip(df['geo_lon'], df['geo_lat'])]",
                "gdf = GeoDataFrame(df, geometry = geometry)   ",
                "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))",
                "gdf.plot(",
                "    ax = world.plot(figsize = (20, 10)),",
                "    marker = '+',",
                "    color = 'red',",
                "    markersize = 15)"
            ]
        },
        {
            "name" : "Draw points on county's map",
            "code" : [
                "from shapely.geometry import Point, Polygon",
                "import geopandas as gpd",
                "from geopandas import GeoDataFrame",
                "",
                "df = pd.read_csv('...')",
                "# https://gadm.org/download_country.html",
                "reg_map = gpd.read_file('example-map/gadm41_RUS_1.shp')",
                "geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]",
                "crs = {'init': 'epsg:4326'}",
                "gdf = GeoDataFrame(df, crs = crs, geometry = geometry)",
                "fig, ax = plt.subplots(figsize = (40, 30))",
                "reg_map.plot(ax = ax)",
                "gdf.plot(",
                "    ax = ax,",
                "    markersize = 10,",
                "    color = 'red',",
                "    marker = '+'',",
                "    label = 'Apartments')",
                "plt.legend(",
                "    prop = {'size': 25},",
                "    loc = 'center left',",
                "    fancybox = True,",
                "    framealpha = 1,",
                "    shadow = False,",
                "    borderpad = 1,",
                "    labelcolor = 'black',",
                "    facecolor = 'lightseagreen',",
                "    edgecolor = 'lightseagreen',",
                "    markerscale = 11)",
                "plt.show()"
            ]
        }
    ]
} 
