{
    "snippets" : [
        {
            "name" : "Warning filters",
            "code" : [
                "import warnings",
                "warnings.filterwarnings('ignore', category = FutureWarning)",
                "warnings.filterwarnings('ignore', category = UserWarning)"
            ]
        },
        {
            "name" : "Compress notebook",
            "code" : [
                "import os",
                "from ipynbcompress import compress",
                "",
                "def compress_notebook(file_in, file_out):",
                "    old_size = os.stat(file_in).st_size",
                "    new_size = compress(file_in, output_filename = file_out, img_format = 'jpeg')",
                "    return old_size - new_size"
            ]
        },
        {
            "name" : "Images quality tweaks",
            "code" : [
                "# image format",
                "%config InlineBackend.figure_format = 'jpg'",
                "# dpi tweak (default - 100)",
                "matplotlib.rcParams['figure.dpi'] = 100",
                "# plot size tweak",
                "matplotlib.pyplot.rcParams['figure.figsize'] = (10, 3) # makes figures larger"
            ]
        },
        {
            "name" : "Pandas, NumPy, Matplotlib, Seaborn, etc",
            "code" : [
                "import numpy as np",
                "import pandas as pd",
                "pd.set_option('display.max_rows', 50)",
                "pd.set_option('display.max_columns', 10)",
                "import matplotlib.pyplot as plt",
                "import seaborn as sb",
                "import missingno as msno",
                "import random as rd"
            ]
        },
        {
            "name" : "Scikit-learn",
            "code" : [
                "from sklearn.metrics import confusion_matrix, classification_report",
                "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder",
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "name" : "Classifiers (Scikit-learn, CatBoost, XGBoost)",
            "code" : [
                "from sklearn.model_selection import GridSearchCV",
                "from sklearn.neighbors import KNeighborsClassifier",
                "from sklearn.tree import DecisionTreeClassifier",
                "from sklearn.linear_model import LogisticRegression",
                "from sklearn.linear_model import SGDClassifier",
                "from sklearn.svm import LinearSVC",
                "from sklearn.svm import SVC",
                "from sklearn.ensemble import RandomForestClassifier",
                "from sklearn.ensemble import ExtraTreesClassifier",
                "from sklearn.ensemble import BaggingClassifier",
                "from sklearn.ensemble import AdaBoostClassifier",
                "from sklearn.ensemble import GradientBoostingClassifier",
                "from sklearn.ensemble import HistGradientBoostingClassifier",
                "from sklearn.ensemble import VotingClassifier",
                "from sklearn.naive_bayes import GaussianNB",
                "from catboost import CatBoostClassifier",
                "from xgboost import XGBClassifier"
            ]
        },
        {
            "name" : "Regressors & metrics (Scikit-learn, CatBoost, XGBoost)",
            "code" : [
                "from sklearn.neighbors import KNeighborsRegressor",
                "from sklearn.linear_model import LinearRegression",
                "from sklearn.tree import DecisionTreeRegressor",
                "from sklearn.ensemble import RandomForestRegressor",
                "from sklearn.ensemble import ExtraTreesRegressor",
                "from sklearn.ensemble import BaggingRegressor",
                "from sklearn.ensemble import AdaBoostRegressor",
                "from sklearn.ensemble import HistGradientBoostingRegressor",
                "from catboost import CatBoostRegressor",
                "from xgboost import XGBRegressor",
                "from sklearn.model_selection import GridSearchCV",
                "from sklearn.metrics import r2_score",
                "from sklearn.metrics import mean_absolute_error",
                "from sklearn.metrics import mean_squared_error"
            ]
        },
        {
            "name" : "TF & Keras",
            "code" : [
                "import tensorflow as tf",
                "from tensorflow import keras",
                "from tensorflow.keras.utils import to_categorical",
                "from keras.callbacks import EarlyStopping, ModelCheckpoint",
                "from keras.models import Sequential",
                "from keras.layers import Dense, Flatten, Dropout, Conv2D, BatchNormalization, ReLU",
                "from keras.layers import MaxPooling2D, AveragePooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D",
                "# Image preprocessing",
                "from keras.layers import Rescaling, Resizing, CenterCrop",
                "# Image augmentation",
                "from keras.preprocessing.image import ImageDataGenerator",
                "from keras.layers import RandomCrop, RandomFlip, RandomTranslation, RandomRotation, RandomZoom, RandomContrast",
                "",
                "# List of devices",
                "tf.config.list_physical_devices()",
                "",
                "# Plot model function",
                "# keras.utils.plot_model(model, show_shapes = True)",
                "",
                "# Model loading function",
                "# model = keras.models.load_model('model.keras', compile = False)",
                "# model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])",
                "# model.summary()"
            ]
        },
        {
            "name" : "PyTorch",
            "code" : [
                "import torch",
                "",
                "torch.cuda.is_available()"
            ]
        },
        {
            "name" : "MNIST digits dataset (Scikit-learn)",
            "code" : [
                "from sklearn.datasets import fetch_openml",
                "from sklearn.preprocessing import LabelEncoder",
                "",
                "data = fetch_openml('mnist_784', version = 1, cache = True, parser = 'auto')",
                "x = data['data']",
                "y = data['target']",
                "",
                "encoder = LabelEncoder()",
                "encoder.fit(y)",
                "print(encoder.inverse_transform([0]))",
                "print(encoder.classes_)"
            ]
        },
        {
            "name" : "MNIST digits dataset (Keras)",
            "code" : [
                "from tensorflow.keras.datasets import mnist",
                "from sklearn.preprocessing import LabelEncoder",
                "",
                "(x_train, y_train), (x_test, y_test) = mnist.load_data()",
                "",
                "encoder = LabelEncoder()",
                "encoder.fit(y_train)",
                "print(encoder.inverse_transform([0]))",
                "print(encoder.classes_)"
            ]
        },
        {
            "name" : "EMNIST digits & letters dataset",
            "code" : [
                "from extra_keras_datasets import emnist",
                "from sklearn.preprocessing import LabelEncoder",
                "",
                "(x_train, y_train), (x_test, y_test) = emnist.load_data(type = 'balanced')",
                "",
                "# labels for the balanced dataset, https://arxiv.org/pdf/1702.05373v1.pdf",
                "labels = [str(i) for i in range(0, 10)]",
                "labels += ['a', 'b', 'd', 'e', 'f', 'g', 'h', 'n', 'q', 'r', 't']",
                "labels += [chr(i) for i in range(65, 91)]",
                "",
                "encoder = LabelEncoder()",
                "encoder.fit(labels)",
                "print(encoder.inverse_transform([0]))",
                "print(encoder.classes_)"
            ]
        },
        {
            "name" : "CIFAR-10 dataset",
            "code" : [
                "from keras.datasets import cifar10",
                "from sklearn.preprocessing import LabelEncoder",
                "",
                "(x_train, y_train), (x_test, y_test) = cifar10.load_data()",
                "",
                "labels = ['airplane', 'automobile', 'bird',",
                "          'cat', 'deer', 'dog', 'frog',",
                "          'horse', 'ship', 'truck']",
                "encoder = LabelEncoder()",
                "encoder.fit(labels)",
                "print(encoder.inverse_transform([0]))",
                "print(encoder.classes_)"
            ]
        },
        {
            "name" : "Image dataset showcase",
            "code" : [
                "def showcase_image_dataset(x, y, encoder = None):",
                "    plt.subplots(4, 5, figsize = (8, 5))",
                "    plt.tight_layout()",
                "    for i in range(0, 20):",
                "        n = np.random.randint(0, x.shape[0])",
                "        plt.subplot(4, 5, i + 1)",
                "        plt.axis('off')",
                "        if encoder == None:",
                "            plt.gca().set_title(f'{y[n]}')",
                "        else:",
                "            plt.gca().set_title(f'{encoder.inverse_transform([y[n]])[0]} ({y[n]})')",
                "        plt.imshow(x[n])",
                "    plt.show()"
            ]
        },
        {
            "name" : "Show class distribution",
            "code" : [
                "def show_class_distribution(y, encoder = None):",
                "    plt.figure(figsize = (10, 4))",
                "    plt.title('Class distribution')",
                "    if encoder != None:",
                "        sb.histplot(encoder.inverse_transform(y), discrete = True)",
                "    else:",
                "        sb.histplot(y, discrete = True)",
                "    plt.show()"
            ]
        },
        {
            "name" : "Show confusion matrix",
            "code" : [
                "def show_confusion_matrix(labels_test, labels_predicted, labels = None):", 
                "    plt.figure(figsize = (10, 7))",
                "    cm = confusion_matrix(labels_test, labels_predicted)",
                "    if labels != None:",
                "        sb.heatmap(",
                "            cm,",
                "            annot = True,",
                "            fmt = 'd',",
                "            cbar = False,",
                "            xticklabels = labels,",
                "            yticklabels = labels)",
                "    else:",
                "        sb.heatmap(",
                "            cm,",
                "            annot = True,",
                "            fmt = 'd',",
                "            cbar = False)",
                "    plt.xlabel('Predicted')",
                "    plt.ylabel('Truth')",
                "    plt.show()"
            ]
        },
        {
            "name" : "Show regression chart",
            "code" : [
                "def show_regression_chart(y, prediction, sort = True):",
                "    t = pd.DataFrame({'prediction': prediction, 'y': y})",
                "    if sort:",
                "        t.sort_values(by = ['y'], inplace = True)",
                "    plt.figure(figsize = (10, 6))",
                "    plt.plot(t['prediction'].tolist(), label = 'prediction')",
                "    plt.plot(t['y'].tolist(), label = 'expected')",
                "    plt.ylabel('output')",
                "    plt.legend()",
                "    plt.show()"
            ]
        },
        {
            "name" : "Show training history (classification)",
            "code" : [
                "def show_training_history_classification(history, last_epoch = None):",
                "    fig, ax = plt.subplots(2, 1)",
                "    ax[0].plot(history.history['loss'], color = 'b', label = 'Training loss')",
                "    if 'val_loss' in history.history:",
                "        ax[0].plot(history.history['val_loss'], color = 'r', label = 'Validation loss', axes = ax[0])",
                "    if last_epoch is not None:",
                "        ax[0].axvline(x = last_epoch, color = 'g', label = 'Start of fine tuning')",
                "    legend = ax[0].legend(loc = 'best', shadow = True)",
                "    ax[1].plot(history.history['accuracy'], color = 'b', label = 'Training accuracy')",
                "    if 'val_accuracy' in history.history:",
                "        ax[1].plot(history.history['val_accuracy'], color = 'r',label = 'Validation accuracy')",
                "    if last_epoch is not None:",
                "        ax[1].axvline(x = last_epoch, color = 'g', label = 'Start of fine tuning')",
                "    legend = ax[1].legend(loc = 'best', shadow = True)",
                "    plt.show()"
            ]
        },
        {
            "name" : "Show training history (regression)",
            "code" : [
                "def show_training_history_regression(history, last_epoch = None):",
                "    fig, ax = plt.subplots(2, 1)",
                "    ax[0].plot(history.history['loss'], color = 'b', label = 'Training loss')",
                "    if 'val_loss' in history.history:",
                "        ax[0].plot(history.history['val_loss'], color = 'r', label = 'Validation loss', axes = ax[0])",
                "    if last_epoch is not None:",
                "        ax[0].axvline(x = last_epoch, color = 'g', label = 'Start of fine tuning')",
                "    legend = ax[0].legend(loc = 'best', shadow = True)",
                "    ax[1].plot(history.history['mean_squared_error'], color = 'b', label = 'Training MSE')",
                "    if 'val_mean_squared_error' in history.history:",
                "        ax[1].plot(history.history['val_mean_squared_error'], color = 'r',label = 'Validation MSE')",
                "    if last_epoch is not None:",
                "        ax[1].axvline(x = last_epoch, color = 'g', label = 'Start of fine tuning')",
                "    legend = ax[1].legend(loc = 'best', shadow = True)",
                "    plt.show()"
            ]
        },
        {
            "name" : "Remove outliers",
            "code" : [
                "def remove_outliers(df, column):",
                "    q_low = df[column].quantile(0.01)",
                "    q_high = df[column].quantile(0.99)",
                "    return df[(df[column] < q_high) & (df[column] > q_low)]",
                "",
                "def remove_outliers_iqr(df, column):",
                "    q1 = np.percentile(df[column], 25, interpolation = 'midpoint')",
                "    q3 = np.percentile(df[column], 75, interpolation = 'midpoint')",
                "    iqr = q3 - q1",
                "    lower = q1 - 1.5*iqr",
                "    upper = q3 + 1.5*iqr",
                "    upper_array = np.where(df[column] >= upper)[0]",
                "    lower_array = np.where(df[column] <= lower)[0]",
                "    result = df.copy()",
                "    result.drop(index = upper_array, inplace = True)",
                "    result.drop(index = lower_array, inplace = True)",
                "    return result",
                "",
                "def remove_outliers_z_score(df, column):",
                "    result = df.copy()",
                "    result['z_score'] = stats.zscore(df[column])",
                "    result = result.loc[result['z_score'].abs() <= 3]",
                "    return result"
            ]
        },
        {
            "name" : "T-test (example)",
            "code" : [
                "from scipy.stats import ttest_ind",
                "",
                "tval, pval = ttest_ind(",
                "    df.loc[df['Role'] == 'First_Baseman', ['Height']],",
                "    df.loc[df['Role'] == 'Second_Baseman', ['Height']],",
                "    equal_var = False)",
                "print(f'T-value: {tval[0]:.2f}')",
                "print(f'P-value: {pval[0]}')"
            ]
        },
        {
            "name" : "(Pandas) Show top classes",
            "code" : [
                "def show_top_classes(df, column):",
                "    top = df[column].value_counts()",
                "    plt.figure(figsize = (10, 7))",
                "    sb.barplot(x = top.index, y = top.values)",
                "    plt.xticks(rotation = 45)",
                "    plt.title('Top classes', color = 'blue')",
                "    plt.show()"
            ]
        },
        {
            "name" : "(Pandas) Dataset lookup",
            "code" : [
                "def dataset_lookup(df):",
                "    print('Dataset info:')",
                "    print(df.info())",
                "    print()",
                "    print('Dataset description:')",
                "    print(df.describe())",
                "    print()",
                "    print('Amount of duplicates:')",
                "    duplicates_amount = df.duplicated().sum()",
                "    print(duplicates_amount)",
                "    if (duplicates_amount > 0):",
                "        print('Duplicates:')",
                "        print(df[df.duplicated()])",
                "    print()",
                "    print('Amount of missing values:')",
                "    print(df.isna().sum(axis = 0))",
                "    print()",
                "    print('Unique values in each column:')",
                "    object_types_count = 0",
                "    for column in df:",
                "        if df[column].dtypes == 'object':",
                "            object_types_count += 1",
                "            print('-' * len(column))",
                "            print(f'{column}: {df[column].unique()}')",
                "    if object_types_count == 0:",
                "        print('-' * 10)",
                "        print('No object columns in this dataset.')"
            ]
        },
        {
            "name" : "(Pandas) Dataset preprocessing template",
            "code" : [
                "dataset_preprocessed = dataset.copy()",
                "# Renaming of the values in column",
                "dataset_preprocessed['column_name'].replace({1: 'Class_1', 2: 'Class_2'}, inplace = True)",
                "# One-hot encoding",
                "dataset_preprocessed = pd.get_dummies(dataset_preprocessed, columns = ['column_name'])",
                "dataset_preprocessed.replace({False: 0, True: 1}, inplace = True)",
                "# Label encoding",
                "label_encoder = LabelEncoder()",
                "dataset_preprocessed['column'] = label_encoder.fit_transform(dataset_preprocessed['column'])",
                "# Standardization",
                "dataset_numerical_only = dataset_preprocessed.drop(",
                "    columns = [",
                "        'col_1',",
                "        'col_2',",
                "        'col_3'",
                "    ])",
                "scaler = StandardScaler()",
                "dataset_standardized = pd.DataFrame(",
                "    scaler.fit_transform(dataset_numerical_only),",
                "    columns = [",
                "        'col_1',",
                "        'col_2',",
                "        'col_3',",
                "    ])",
                "dataset_standardized.insert(3, 'old_col_1', dataset_preprocessed['old_col_1'])",
                "dataset_standardized.insert(4, 'old_col_2', dataset_preprocessed['old_col_2'])",
                "dataset_standardized"
            ]
        },
        {
            "name" : "(Pandas) Visualization functions",
            "code" : [
                "def plot_dataset(df):",
                "    df.plot()",
                "    plt.show()",
                "",
                "def plot_dataset_histograms(df):",
                "    df.hist()",
                "    plt.show()",
                "",
                "def show_area_plot(df):",
                "    df.plot.area()",
                "    plt.show()",
                "",   
                "def show_proportional_area_plot(df):",
                "    dfp = df.select_dtypes(exclude = ['object']).iloc[:, :].apply(lambda x: x / x.sum(), axis = 1)",
                "    dfp.plot.area()",
                "    plt.show()",
                "",    
                "def show_correlation_map(df, figure_size = (15, 15)):",
                "    plt.figure(figsize = figure_size)",
                "    sb.heatmap(",
                "        df.select_dtypes(exclude = ['object']).corr(),",
                "        annot = True,",
                "        cmap = 'coolwarm',",
                "        fmt = '.2f')",
                "    plt.show()",
                "",
                "def show_pair_plot(df, figure_size = (15, 15), target_name = None):",
                "    plt.figure(figsize = figure_size)",
                "    df_without_object_columns = df.select_dtypes(exclude = ['object'])",
                "    if target_name != None:",
                "        sb.pairplot(df_without_object_columns, palette = 'tab10', hue = target_name)",
                "    else:",
                "        sb.pairplot(df_without_object_columns)",
                "    plt.show()",
                "",    
                "def show_cat_plot(df):",
                "    sb.catplot(df, kind = 'boxen', height = 8)",
                "    plt.show()"
            ]
        },
        {
            "name" : "(Pandas) Dataframe manipulation functions",
            "code" : [
                "def transpose_dataset(df):",
                "    return df.describe().transpose()",
                "",
                "def replace_bool_with_int(df):",
                "    return df.replace({False: 0, True: 1})",
                "",
                "def drop_rows_range(df, start_index, end_index):",
                "    df_dropped = df.drop(dataset.index[start_index:end_index])",
                "    return df_dropped",
                "",
                "def rebuild_index_in_dataset(df):",
                "    return df.reset_index().drop(columns = ['index'])",
                "",
                "def add_rows_to_dataset(df, dataframe_rows):",
                "    return pd.concat([df] + dataframe_rows, ignore_index = True)",
                "",
                "def get_row_by_index(df, index):",
                "    return df.iloc[[index]]"
            ]
        },
        {
            "name" : "Dataset split",
            "code" : [
                "from sklearn.model_selection import train_test_split",
                "from tensorflow.keras.utils import to_categorical",
                "",
                "x_train_, x_test, y_train_, y_test = train_test_split(x, y, test_size = 0.25, random_state = 42, shuffle = True)",
                "x_train, x_validation, y_train, y_validation = train_test_split(x_train_, y_train_, test_size = 0.1, random_state = 42)",
                "",
                "y_train_categorical = to_categorical(y_train)",
                "y_test_categorical = to_categorical(y_test)",
                "y_validation_categorical = to_categorical(y_validation)",
                "",
                "print(f'x train: {x_train.shape}')",
                "print(f'y train: {y_train.shape}')",
                "print(f'y train categorical: {y_train_categorical.shape}')",
                "print()",
                "print(f'x test: {x_test.shape}')",
                "print(f'y test: {y_test.shape}')",
                "print(f'y test categorical: {y_test_categorical.shape}')",
                "print()",
                "print(f'x validation: {x_validation.shape}')",
                "print(f'y validation: {y_validation.shape}')",
                "print(f'y validation categorical: {y_validation_categorical.shape}')"
            ]
        },
        {
            "name" : "One-hot encoding",
            "code" : [
                "dataset_encoded = pd.get_dummies(dataset, columns = ['column'])",
                "dataset_encoded.replace({False: 0, True: 1}, inplace = True)",
                "dataset_encoded"
            ]
        },
        {
            "name" : "Label encoding",
            "code" : [
                "from sklearn.preprocessing import LabelEncoder",
                "",
                "label_encoder = LabelEncoder()",
                "dataset_encoded = dataset.copy()",
                "dataset_encoded['column'] = label_encoder.fit_transform(dataset_encoded['column'])",
                "dataset_encoded"
            ]
        },
        {
            "name" : "LassoCV",
            "code" : [
                "from numpy import arange",
                "from sklearn.linear_model import LassoCV",
                "from sklearn.model_selection import RepeatedKFold",
                "",
                "cv = RepeatedKFold(n_splits = 10, n_repeats = 3, random_state = 1)",
                "model = LassoCV(alphas = arange(0.1, 10, 0.1), cv = cv, n_jobs = -1)",
                "model.fit(x, y)",
                "",
                "print(model.alpha_)"
            ]
        },
        {
            "name" : "ROC curve (two classes)",
            "code" : [
                "from sklearn.metrics import roc_curve, roc_auc_score",
                "",
                "y_pred_proba = model.predict_proba(x_test)[:, 1]",
                "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)",
                "roc_auc = roc_auc_score(y_test, y_pred_proba)",
                "# Plot the ROC curve",
                "plt.plot(fpr, tpr, label = f'ROC curve (area = {roc_auc})')",
                "# ROC curve for tpr = fpr",
                "plt.plot([0, 1], [0, 1], 'k--', label = 'Classifier')",
                "plt.xlabel('False positive Rate')",
                "plt.ylabel('True positive Rate')",
                "plt.title('ROC curve')",
                "plt.legend(loc = 'lower right')",
                "plt.show()"
            ]
        },
        {
            "name" : "ROC curve (multiple classes)",
            "code" : [
                "from sklearn.metrics import roc_curve, roc_auc_score",
                "from sklearn.multiclass import OneVsRestClassifier",
                "",
                "oneVsRest = OneVsRestClassifier(SomeModel(random_state = 42))",
                "oneVsRest.fit(x_train, y_train)",
                "y_pred_proba = oneVsRest.predict_proba(x_test)",
                "roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class = 'ovr')",
                "print(f'ROC-AUC Score: {roc_auc}')",
                "colors = ['red', 'green', 'blue', 'yellow']",
                "classes = ['0', '1', '2', '3']",
                "for i in range(len(classes)): ",
                "    fpr, tpr, thresh = roc_curve(y_test, y_pred_proba[:, i], pos_label = i)",
                "    plt.plot(fpr, tpr, linestyle = '--', color = colors[i], label = f'{classes[i]} vs Rest')",
                "# ROC curve for tpr = fpr",
                "plt.plot([0, 1], [0, 1], 'k--', label = 'Classifier')",
                "plt.title('Multiclass ROC curve')",
                "plt.xlabel('False positive Rate')",
                "plt.ylabel('True positive rate')",
                "plt.legend()",
                "plt.show()"
            ]
        },
        {
            "name" : "Grid search CV (classification)",
            "code" : [
                "from sklearn.model_selection import GridSearchCV",
                "",
                "parameters = {",
                "    'max_depth': [4, 8, 16, 32]",
                "}",
                "",
                "clf = RandomForestClassifier(random_state = 42)",
                "clf.fit(x_train, y_train)",
                "gs = GridSearchCV(estimator = clf, cv = 5, param_grid = parameters, n_jobs = -1, verbose = 4)",
                "gs.fit(x_train, y_train)",
                "print(f'Best parameters: {gs.best_params_}')",
                "clf.set_params(**gs.best_params_)",
                "y_pred = clf.predict(x_test)",
                "print(classification_report(y_test, y_pred))",
                "show_confusion_matrix(y_test, y_pred)"
            ]
        },
        {
            "name" : "Grid search CV (regression)",
            "code" : [
                "from sklearn.model_selection import GridSearchCV",
                "from sklearn.metrics import r2_score",
                "from sklearn.metrics import mean_absolute_error",
                "from sklearn.metrics import mean_squared_error",
                "",
                "parameters = {",
                "    'max_depth': [4, 8, 16, 32]",
                "}",
                "",
                "reg = RandomForestRegressor(random_state = 42)",
                "reg.fit(x_train, y_train)",
                "gs = GridSearchCV(estimator = reg, scoring = 'r2', cv = 5, param_grid = parameters, n_jobs = -1, verbose = 4)     ",
                "gs.fit(x_train, y_train)",
                "print(f'Best parameters: {gs.best_params_}')",
                "reg.set_params(**gs.best_params_)",
                "y_pred = reg.predict(x_test)",
                "print(f'R2 score: {r2_score(y_test, y_pred)}')",
                "print(f'MAE error: {mean_absolute_error(y_test, y_pred)}')",
                "print(f'MSE error: {mean_squared_error(y_test, y_pred)}')",
                "print(f'RMSE error: {np.sqrt(mean_squared_error(y_test, y_pred))}')"
            ]
        },
        {
            "name" : "Cross-validation using ML classifiers",
            "code" : [
                "from sklearn.model_selection import StratifiedKFold, cross_val_score",
                "from sklearn.linear_model import LogisticRegression",
                "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis",
                "from sklearn.neighbors import KNeighborsClassifier",
                "from sklearn.naive_bayes import GaussianNB",
                "from sklearn.tree import DecisionTreeClassifier",
                "from sklearn.ensemble import RandomForestClassifier",
                "from sklearn.ensemble import ExtraTreesClassifier",
                "from sklearn.ensemble import BaggingClassifier",
                "from sklearn.ensemble import AdaBoostClassifier",
                "from sklearn.ensemble import HistGradientBoostingClassifier",
                "from xgboost import XGBClassifier",
                "from catboost import CatBoostClassifier",
                "",
                "models = []",
                "models.append(('LR', LogisticRegression(random_state = 42, n_jobs = -1)))",
                "models.append(('LDA', LinearDiscriminantAnalysis()))",
                "models.append(('KNN', KNeighborsClassifier(n_neighbors = 20, n_jobs = -1)))",
                "models.append(('NB', GaussianNB()))",
                "models.append(('CART', DecisionTreeClassifier(random_state = 42)))",
                "models.append(('RF', RandomForestClassifier(n_jobs = -1, random_state = 42)))",
                "models.append(('XT', ExtraTreesClassifier(n_jobs = -1, random_state = 42)))",
                "models.append(('BG', BaggingClassifier(n_jobs = -1, random_state = 42)))",
                "models.append(('ADA', AdaBoostClassifier(random_state = 42)))",
                "models.append(('HG', HistGradientBoostingClassifier(random_state = 42)))",
                "models.append(('XGB', XGBClassifier(tree_method = 'gpu_hist', verbosity = 0, random_state = 42)))",
                "models.append(('CAT', CatBoostClassifier(",
                "    random_seed = 42,",
                "    loss_function = 'MultiClass',",
                "    eval_metric = 'Accuracy',",
                "    od_type = 'Iter',",
                "    od_wait = 20,",
                "    task_type = 'GPU',",
                "    logging_level = 'Silent')))",
                "",
                "results = []",
                "names = []",
                "for name, model in models:",
                "    kfold = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)",
                "    cv_results = cross_val_score(model, x, y, cv = kfold, scoring = 'accuracy')",
                "    results.append(cv_results)",
                "    names.append(name)",
                "    print(f'{name}: mean - {cv_results.mean()}; std - {cv_results.std()}')",
                "plt.boxplot(results, labels = names)",
                "plt.title('Algorithms comparison')",
                "plt.show()"
            ]
        },
        {
            "name" : "Cross-validation using ML regressors",
            "code" : [
                "from sklearn.model_selection import KFold, cross_val_score",
                "from sklearn.linear_model import LinearRegression",
                "from sklearn.neighbors import KNeighborsRegressor",
                "from sklearn.tree import DecisionTreeRegressor",
                "from sklearn.ensemble import RandomForestRegressor",
                "from sklearn.ensemble import ExtraTreesRegressor",
                "from sklearn.ensemble import BaggingRegressor",
                "from sklearn.ensemble import AdaBoostRegressor",
                "from sklearn.ensemble import HistGradientBoostingRegressor",
                "from xgboost import XGBRegressor",
                "from catboost import CatBoostRegressor",
                "",
                "models = []",
                "models.append(('LR', LinearRegression(n_jobs = -1)))",
                "models.append(('KNN', KNeighborsRegressor(n_neighbors = 20, n_jobs = -1)))",
                "models.append(('CART', DecisionTreeRegressor(random_state = 42)))",
                "models.append(('RF', RandomForestRegressor(n_jobs = -1, random_state = 42)))",
                "models.append(('XT', ExtraTreesRegressor(n_jobs = -1, random_state = 42)))",
                "models.append(('BG', BaggingRegressor(n_jobs = -1, random_state = 42)))",
                "models.append(('ADA', AdaBoostRegressor(random_state = 42)))",
                "models.append(('HG', HistGradientBoostingRegressor(random_state = 42)))",
                "models.append(('XGB', XGBRegressor(tree_method = 'gpu_hist', verbosity = 0, random_state = 42)))",
                "models.append(('CAT', CatBoostRegressor(",
                "    random_seed = 42,",
                "    loss_function = 'RMSE',",
                "    eval_metric = 'R2',",
                "    od_type = 'Iter',",
                "    od_wait = 20,",
                "    task_type = 'GPU',",
                "    logging_level = 'Silent')))",
                "",
                "results = []",
                "names = []",
                "for name, model in models:",
                "    kfold = KFold(n_splits = 5, random_state = 42, shuffle = True)",
                "    cv_results = cross_val_score(model, x, y, cv = kfold, scoring = 'r2')",
                "    #cv_results = cross_val_score(model, x, y, cv = kfold, scoring = 'neg_mean_squared_error')",
                "    results.append(cv_results)",
                "    names.append(name)",
                "    print(f'{name}: mean - {cv_results.mean()}; std - {cv_results.std()}')",
                "plt.boxplot(results, labels = names)",
                "plt.title('Algorithms comparison')",
                "plt.show()"
            ]
        },
        {
            "name" : "Cross-validation using Keras (classification)",
            "code" : [
                "from sklearn.model_selection import StratifiedKFold",
                "",
                "# labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']",
                "# encoder = LabelEncoder()",
                "# encoder.fit(labels)",
                "",
                "batch_size = 32",
                "epochs = 5",
                "num_folds = 5",
                "acc_per_fold = []",
                "loss_per_fold = []",
                "histories = []",
                "kfold = StratifiedKFold(n_splits = num_folds, random_state = 42, shuffle = True)",
                "fold_no = 1",
                "for train, test in kfold.split(x, y):",
                "    # Perceptron",
                "    '''",
                "    model = Sequential(",
                "    [",
                "        Input(shape = x_train.shape[1]),",
                "        Dense(50, activation = 'relu'),",
                "        Dropout(0.20),",
                "        Dense(50, activation = 'relu'),",
                "        Dropout(0.25),",
                "        Dense(50, activation = 'relu'),",
                "        Dropout(0.3),",
                "        Dense(num_classes, activation = 'softmax')",
                "    ])",
                "    '''",
                "    # CNN",
                "    model = Sequential(",
                "    [",
                "        # Preprocessing",
                "        Rescaling(1.0/255.0, input_shape = (img_width, img_height, channels)),",
                "        # Augmentation",
                "        RandomFlip('horizontal_and_vertical'),",
                "        RandomRotation(0.8),",
                "        RandomZoom(height_factor = (0.0, -0.3), width_factor = (0.0, -0.3)),",
                "        # Convolution",
                "        Conv2D(32, kernel_size = (3, 3), activation = 'relu', input_shape = (img_width, img_height, channels)),",
                "        MaxPooling2D(pool_size = (2, 2)),",
                "        Conv2D(64, kernel_size = (3, 3), activation = 'relu')",
                "        MaxPooling2D(pool_size = (2, 2)),",
                "        Flatten(),",
                "        Dense(128, activation = 'relu'),",
                "        Dropout(0.5),",
                "        Dense(64, activation = 'relu'),",
                "        Dense(num_classes, activation = 'softmax')",
                "    ])",
                "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])",
                "    print('---')",
                "    print(f'Training for fold {fold_no}...')",
                "    history = model.fit(",
                "        x = x[train],",
                "        y = y[train],",
                "        batch_size = batch_size,",
                "        epochs = epochs)",
                "    histories.append(history)",
                "    scores = model.evaluate(x[test], y[test], verbose = 0)",
                "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1] * 100}%')",
                "    acc_per_fold.append(scores[1])",
                "    loss_per_fold.append(scores[0])",
                "    show_training_history_classification(history)",
                "    y_pred = np.argmax(model.predict(x[test], verbose = 0), axis = 1)",
                "    print(classification_report(",
                "        encoder.inverse_transform(y[test]),",
                "        encoder.inverse_transform(y_pred)))",
                "    show_confusion_matrix(y[test], y_pred, labels)",
                "    fold_no = fold_no + 1",
                "print('---')",
                "print('Score per fold')",
                "for i in range(0, len(acc_per_fold)):",
                "    print('---')",
                "    print(f'> Fold {i + 1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')",
                "print('---')",
                "print('Average scores for all folds:')",
                "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')",
                "print(f'> Loss: {np.mean(loss_per_fold)}')",
                "print('---')",
                "metrics = [loss_per_fold, acc_per_fold]",
                "plt.boxplot(metrics, labels = ['Loss', 'Accuracy'])",
                "plt.title('Metrics')",
                "plt.show()"
            ]
        },
        {
            "name" : "Cross-validation using Keras (regression)",
            "code" : [
                "from sklearn.model_selection import KFold",
                "",
                "batch_size = 32",
                "epochs = 5",
                "num_folds = 5",
                "mse_per_fold = []",
                "rmse_per_fold = []",
                "mae_per_fold = []",
                "r2_per_fold = []",
                "loss_per_fold = []",
                "histories = []",
                "kfold = KFold(n_splits = num_folds, random_state = 42, shuffle = True)",
                "fold_no = 1",
                "for train, test in kfold.split(x, y):",
                "    model = Sequential(",
                "    [",
                "        Input(shape = x_train.shape[1]),",
                "        Dense(50, activation = 'relu'),",
                "        Dropout(0.20),",
                "        Dense(50, activation = 'relu'),",
                "        Dropout(0.25),",
                "        Dense(50, activation = 'relu'),",
                "        Dropout(0.3),",
                "        Dense(1)",
                "    ])",
                "    model.compile(loss = 'mean_absolute_error', optimizer = 'adam', metrics = ['mean_squared_error'])",
                "    print('---')",
                "    print(f'Training for fold {fold_no}...')",
                "    history = model.fit(",
                "        x = x.iloc[train],",
                "        y = y[train],",
                "        batch_size = batch_size,",
                "        epochs = epochs)",
                "    histories.append(history)",
                "    scores = model.evaluate(x.iloc[test], y[test], verbose = 0)",
                "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]}')",
                "    mse_per_fold.append(scores[1])",
                "    loss_per_fold.append(scores[0])",
                "    show_training_history_regression(history)",
                "    y_pred = model.predict(x.iloc[test], verbose = 0)",
                "    y_pred = y_pred.flatten()",
                "    r2 = r2_score(y[test], y_pred)",
                "    mae = mean_absolute_error(y[test], y_pred)",
                "    rmse = np.sqrt(mean_squared_error(y[test], y_pred))",
                "    print(f'R2 score: {r2}')",
                "    print(f'MAE error: {mae}')",
                "    print(f'RMSE error: {rmse}')",
                "    r2_per_fold.append(r2)",
                "    mae_per_fold.append(mae)",
                "    rmse_per_fold.append(rmse)",
                "    show_regression_chart(y[test], y_pred)",
                "    fold_no = fold_no + 1",
                "print('---')",
                "print('Score per fold')",
                "for i in range(0, len(mse_per_fold)):",
                "    print('---')",
                "    print(f'> Fold {i + 1} - Loss: {loss_per_fold[i]} - MSE: {mse_per_fold[i]}')",
                "print('---')",
                "print('Average scores for all folds:')",
                "print(f'> MSE: {np.mean(mse_per_fold)} (+- {np.std(mse_per_fold)})')",
                "print(f'> RMSE: {np.mean(rmse_per_fold)} (+- {np.std(rmse_per_fold)})')",
                "print(f'> MAE: {np.mean(mae_per_fold)} (+- {np.std(mae_per_fold)})')",
                "print(f'> R2: {np.mean(r2_per_fold)} (+- {np.std(r2_per_fold)})')",
                "print(f'> Loss: {np.mean(loss_per_fold)}')",
                "print('---')",
                "metrics = [",
                "    loss_per_fold,",
                "    mse_per_fold,",
                "    rmse_per_fold,",
                "    mae_per_fold,",
                "    r2_per_fold]",
                "plt.boxplot(metrics, labels = ['Loss', 'MSE', 'RMSE', 'MAE', 'R2'])",
                "plt.title('Metrics')",
                "plt.show()"
            ]
        },
        {
            "name" : "Nested array example",
            "code" : [
                "nested_list = [",
                "    [",
                "        [",
                "            [1.0, 6.0, 7.0, 2.0],",
                "            [2.0, 16.0, 99.0, 23.0],",
                "            [3.0, 3.0, 1.0, 0.0]",
                "        ],",
                "        [",
                "            [5.0, 8.0, 7.0, 11.0],",
                "            [6.0, 16.0, 69.0, 453.0],",
                "            [7.0, 3.0, 7.0, 22.0]",
                "        ]",
                "    ],",
                "    [",
                "        [",
                "            [13.0, 8.0, 7.0, 11.0],",
                "            [14.0, 16.0, 69.0, 453.0],",
                "            [16.0, 3.0, 7.0, 32.0]",
                "        ],",
                "        [",
                "            [17.0, 8.0, 7.0, 11.0],",
                "            [19.0, 16.0, 69.0, 453.0],",
                "            [20.0, 3.0, 7.0, 220.0]",
                "        ]",
                "    ]",
                "]",
                "",
                "nested_array = np.array(nested_list)",
                "print(nested_array)",
                "print(nested_array.shape)"
            ]
        },
        {
            "name" : "MyMinMaxScaler",
            "code" : [
                "from sklearn.preprocessing import MinMaxScaler",
                "",
                "class MyMinMaxScaler(MinMaxScaler):",
                "    def fit_transform(self, X, y = None):",
                "        x = np.reshape(X, (-1, 1))",
                "        return np.reshape(super().fit_transform(x, y = y), newshape = X.shape)"
            ]
        },
        {
            "name" : "(NumPy) Random images (or random n-dim arrays)",
            "code" : [
                "random_color_images = (np.random.random((amount, width, height, channels)) * 255).astype('uint8')",
                "black_images = np.zeros((amount, width, height, channels)).astype('uint8')"
            ]
        },
        {
            "name" : "Class weights",
            "code" : [
                "from sklearn.utils.class_weight import compute_class_weight",
                "",
                "weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(y), y = y)",
                "weights"
            ]
        },
        {
            "name" : "PCA (Principal component analysis)",
            "code" : [
                "from sklearn.decomposition import PCA",
                "",
                "pca = PCA(n_components = 5)",
                "pca_x = pca.fit_transform(x_scaled)",
                "",
                "per_var = np.round(pca.explained_variance_ratio_ * 100.0, decimals = 4)",
                "labels = [str(i) for i in range(1, len(per_var) + 1)]",
                "print(f'Explained varience: {np.sum(per_var)}')",
                "plt.figure(figsize = (10, 6))",
                "plt.bar(x = range(1, len(per_var) + 1), height = per_var, tick_label = labels)",
                "plt.plot(range(1, len(per_var) + 1), np.cumsum(per_var), c = 'red', label = 'Cumulative explained variance')",
                "plt.legend(loc = 'upper left')",
                "plt.ylabel('Percentage of explained variance')",
                "plt.xlabel('Principal component')",
                "plt.title('Scree plot')",
                "plt.show()"
            ]
        },
        {
            "name" : "Plot decision regions (mlxtend)",
            "code" : [
                "from sklearn.ensemble import RandomForestClassifier",
                "from sklearn.linear_model import LogisticRegression",
                "from mlxtend.plotting import plot_decision_regions",
                "",
                "selected_x_train = x_train[:, [0, 1]]",
                "#clf = LogisticRegression(random_state = 42)",
                "clf = RandomForestClassifier(random_state = 42)",
                "clf.fit(selected_x_train, y_train)",
                "",
                "plot_decision_regions(selected_x_train, y_train, clf)",
                "plt.show()"
            ]
        },
        {
            "name" : "(Dataprep) Automatic EDA",
            "code" : [
                "from dataprep.eda import plot, plot_correlation, create_report, plot_missing",
                "import pandas as pd",
                "",
                "data = pd.read_csv('...')",
                "# Plot columns",
                "plot(data)",
                "# Plot and explore one of the columns",
                "plot(data, 'column')",
                "# Correlations plots",
                "plot_correlation(data)",
                "# Explore missing data",
                "plot_missing(data)",
                "# Create report based on the dataset",
                "create_report(data)"
            ]
        },
        {
            "name" : "My synthetic dataset",
            "code" : [
                "def my_random():",
                "    return np.round(np.random.random() * 100 + 0.5)",
                "",
                "def make_synthetic_dataset(rows_count, columns_count):",
                "    if rows_count <= 0 or columns_count <= 0:",
                "        raise('Invalid rows/columns count')",
                "    xs = []",
                "    for i in range(columns_count):",
                "        xs.append(np.random.normal(loc = my_random(), scale = my_random(), size = rows_count))",
                "    y = np.random.normal(loc = my_random(), scale = my_random(), size = rows_count)",
                "    xs.append(y)",
                "    xs = np.array(xs)",
                "    xs = np.transpose(xs)",
                "    return pd.DataFrame(xs, columns = [f'x{i + 1}' for i in range(xs.shape[1] - 1)] + ['y'])"
            ]
        }
    ]
} 
